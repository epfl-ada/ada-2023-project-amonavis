{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f521180547c8733",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Why?\n",
    "Idea of this notebook is to give the template that will be run on how to get all of the machine data.\n",
    "\n",
    "Namely, finding the paths and explorations that the two models take. \n",
    "Ideally it'll be upgraded to work with different models too, but that's for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5d6d9e704dcf5c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:20.256519900Z",
     "start_time": "2023-12-20T18:48:15.787754600Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "import data_readers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "# networkx\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "\n",
    "# For semantic similarity\n",
    "from urllib.parse import unquote\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Python functions in .py file to read data\n",
    "import machine_searchers\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "from tqdm import TqdmWarning\n",
    "warnings.filterwarnings('ignore', category=TqdmWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c52d6a923bfe0dcb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:20.444741900Z",
     "start_time": "2023-12-20T18:48:20.261532900Z"
    }
   },
   "outputs": [],
   "source": [
    "finished_paths = pd.read_csv('../datasets/wikispeedia_paths-and-graph/paths_finished.tsv', sep='\\t', skiprows=15,\n",
    "                                 names=['hashedIpAddress', 'timestamp', \"durationInSec\", 'path', \"rating\"])\n",
    "finished_paths['first_article'] = finished_paths['path'].apply(lambda x: x.split(';')[0])\n",
    "finished_paths['last_article'] = finished_paths['path'].apply(lambda x: x.split(';')[-1])\n",
    "finished_paths['path_length'] = finished_paths['path'].apply(lambda x: len(x.split(';')))\n",
    "finished_paths['date'] = pd.to_datetime(finished_paths['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4531dd977229a8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:20.665454Z",
     "start_time": "2023-12-20T18:48:20.452748600Z"
    }
   },
   "outputs": [],
   "source": [
    "# How many each pair of articles has been visited\n",
    "article_combinations_count = finished_paths.groupby(['first_article', 'last_article']).size().reset_index(name='count')\n",
    "\n",
    "# The mean and std of the path length for each pair of articles\n",
    "article_combinations_stats = finished_paths.groupby(['first_article', 'last_article'])['path_length'].agg(['mean', 'std']).reset_index()\n",
    "article_combinations_stats['std'] = article_combinations_stats['std'].fillna(0)\n",
    "article_combinations_stats.rename(columns={'mean': 'mean_length', 'std': 'std_length'}, inplace=True)\n",
    "\n",
    "# The mean and std of the rating for each pair of articles. \n",
    "# Note that mean and std may be nan if there are nan ratings. We purposely leave them as nan, as we don't want to fill them with 0s or 1s.\n",
    "# Depending on the application, we could change this in the future if neeeded.\n",
    "rating_combinations_stats_rating = finished_paths.groupby(['first_article', 'last_article'])['rating'].agg(['mean', 'std']).reset_index()\n",
    "#rating_combinations_stats_rating['std'] = rating_combinations_stats_rating['std'].fillna(0)\n",
    "mask = rating_combinations_stats_rating['mean'].notnull()\n",
    "rating_combinations_stats_rating.loc[mask, 'std'] = rating_combinations_stats_rating.loc[mask, 'std'].fillna(0)\n",
    "rating_combinations_stats_rating.rename(columns={'mean': 'mean_rating', 'std': 'std_rating'}, inplace=True)\n",
    "\n",
    "# The mean and std of the time for each pair of articles.\n",
    "rating_combinations_stats_time = finished_paths.groupby(['first_article', 'last_article'])['durationInSec'].agg(['mean', 'std']).reset_index()\n",
    "rating_combinations_stats_time['std'] = rating_combinations_stats_time['std'].fillna(0)\n",
    "rating_combinations_stats_time.rename(columns={'mean': 'mean_durationInSec', 'std': 'std_durationInSec'}, inplace=True)\n",
    "\n",
    "# Merging all the dataframes\n",
    "article_combinations = pd.merge(article_combinations_count, article_combinations_stats, on=['first_article', 'last_article'])\n",
    "article_combinations = pd.merge(article_combinations, rating_combinations_stats_rating, on=['first_article', 'last_article'])\n",
    "article_combinations = pd.merge(article_combinations, rating_combinations_stats_time, on=['first_article', 'last_article'])\n",
    "\n",
    "# The number of unique sources and targets\n",
    "unique_sources = finished_paths['first_article'].value_counts().reset_index()\n",
    "unique_targets = finished_paths['last_article'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341cf1f617a9831f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:20.726460800Z",
     "start_time": "2023-12-20T18:48:20.666453100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                    first_article          last_article  count  mean_length  \\\n0  %E2%82%AC2_commemorative_coins             Irish_Sea      1          3.0   \n1                    10th_century          11th_century      3          2.0   \n2                    10th_century              Banknote      1          5.0   \n3                    10th_century               Country      1          3.0   \n4                    10th_century  Harlem_Globetrotters      2          4.5   \n\n   std_length  mean_rating  std_rating  mean_durationInSec  std_durationInSec  \n0    0.000000     1.000000    0.000000           15.000000           0.000000  \n1    0.000000     2.333333    2.309401            4.333333           1.527525  \n2    0.000000     3.000000    0.000000           48.000000           0.000000  \n3    0.000000     1.000000    0.000000           15.000000           0.000000  \n4    0.707107     2.000000    0.000000           75.000000          24.041631  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>first_article</th>\n      <th>last_article</th>\n      <th>count</th>\n      <th>mean_length</th>\n      <th>std_length</th>\n      <th>mean_rating</th>\n      <th>std_rating</th>\n      <th>mean_durationInSec</th>\n      <th>std_durationInSec</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>%E2%82%AC2_commemorative_coins</td>\n      <td>Irish_Sea</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>15.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10th_century</td>\n      <td>11th_century</td>\n      <td>3</td>\n      <td>2.0</td>\n      <td>0.000000</td>\n      <td>2.333333</td>\n      <td>2.309401</td>\n      <td>4.333333</td>\n      <td>1.527525</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10th_century</td>\n      <td>Banknote</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>48.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10th_century</td>\n      <td>Country</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>15.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10th_century</td>\n      <td>Harlem_Globetrotters</td>\n      <td>2</td>\n      <td>4.5</td>\n      <td>0.707107</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>75.000000</td>\n      <td>24.041631</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_combinations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f71bc131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T19:17:41.307838Z",
     "start_time": "2023-12-20T19:17:41.076838400Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def decode_word(word):\n",
    "    word = word.replace('_', ' ')\n",
    "    return unquote(word)\n",
    "\n",
    "# Function to get embeddings using sentence transformer\n",
    "def get_embedding(text):\n",
    "    #word = decode_word(text)\n",
    "    return model.encode(text, convert_to_tensor=True)\n",
    "\n",
    "# Function to perform L2 normalization on the embeddings\n",
    "def l2_normalize(tensor):\n",
    "    return tensor / tensor.norm(p=2, dim=0, keepdim=True)\n",
    "\n",
    "# Function to calculate semantic similarity between two pieces of text\n",
    "def semantic_similarity(word1, word2):\n",
    "    embedding1 = get_embedding(word1)\n",
    "    embedding2 = get_embedding(word2)\n",
    "\n",
    "    # L2 normalization of the embeddings (to make sure, although embedding should already be normalized)\n",
    "    embedding1_normalized = l2_normalize(embedding1)\n",
    "    embedding2_normalized = l2_normalize(embedding2)\n",
    "\n",
    "    # Compute and return the similarity of normalized tensors\n",
    "    return torch.dot(embedding1_normalized, embedding2_normalized).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33593d5dce82fe85",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:22.340861Z",
     "start_time": "2023-12-20T18:48:21.504860500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the modded a star that returns explored nodes:\n",
      " Found solution for Actor to Japan exploring the following number of nodes: 1\n",
      " Found it in: 0.10400104522705078\n"
     ]
    }
   ],
   "source": [
    "wikispeedia= nx.read_edgelist('../datasets/wikispeedia_paths-and-graph/links.tsv',\n",
    "                              create_using=nx.DiGraph)\n",
    "\n",
    "def decode_word(word):\n",
    "    word = word.replace('_', ' ')\n",
    "    return unquote(word)\n",
    "\n",
    "# Create a new graph with decoded node labels\n",
    "decoded_wikispeedia = nx.DiGraph()\n",
    "\n",
    "for node in wikispeedia.nodes():\n",
    "    decoded_node = decode_word(node)\n",
    "    decoded_wikispeedia.add_node(decoded_node)\n",
    "\n",
    "# Copy the edges from the original graph to the new graph with decoded node labels\n",
    "for edge in wikispeedia.edges():\n",
    "    decoded_edge = tuple(decode_word(node) for node in edge)\n",
    "    decoded_wikispeedia.add_edge(*decoded_edge)\n",
    "\n",
    "start_time = time.time()\n",
    "lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(wikispeedia, 'Actor', 'Japan', heuristic=semantic_similarity)\n",
    "end_time = time.time()\n",
    "\n",
    "# It's len - 1 because the target node is also included, and that node wasn't explored\n",
    "print(\"Using the modded a star that returns explored nodes:\")\n",
    "print(\" Found solution for Actor to Japan exploring the following number of nodes:\", len(lib_explore_1)-1)\n",
    "print(\" Found it in:\", end_time-start_time)\n",
    "\n",
    "# start_time = time.time()\n",
    "# lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(wikispeedia, 'Actor', 'Japan', heuristic=semantic_similarity)\n",
    "# end_time = time.time()\n",
    "# \n",
    "# print(\"Using depth first only A star that returns explored nodes:\")\n",
    "# print(\" Found solution for Actor to Japan exploring the following number of nodes:\", len(lib_explore_2)-1)\n",
    "# print(\" Found it in:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2af62950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:22.422858800Z",
     "start_time": "2023-12-20T18:48:22.342860100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            first_article          last_article  count  mean_length  \\\n0  €2 commemorative coins             Irish Sea      1          3.0   \n1            10th century          11th century      3          2.0   \n2            10th century              Banknote      1          5.0   \n3            10th century               Country      1          3.0   \n4            10th century  Harlem Globetrotters      2          4.5   \n\n   std_length  mean_rating  std_rating  mean_durationInSec  std_durationInSec  \n0    0.000000     1.000000    0.000000           15.000000           0.000000  \n1    0.000000     2.333333    2.309401            4.333333           1.527525  \n2    0.000000     3.000000    0.000000           48.000000           0.000000  \n3    0.000000     1.000000    0.000000           15.000000           0.000000  \n4    0.707107     2.000000    0.000000           75.000000          24.041631  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>first_article</th>\n      <th>last_article</th>\n      <th>count</th>\n      <th>mean_length</th>\n      <th>std_length</th>\n      <th>mean_rating</th>\n      <th>std_rating</th>\n      <th>mean_durationInSec</th>\n      <th>std_durationInSec</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>€2 commemorative coins</td>\n      <td>Irish Sea</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>15.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10th century</td>\n      <td>11th century</td>\n      <td>3</td>\n      <td>2.0</td>\n      <td>0.000000</td>\n      <td>2.333333</td>\n      <td>2.309401</td>\n      <td>4.333333</td>\n      <td>1.527525</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10th century</td>\n      <td>Banknote</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>48.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10th century</td>\n      <td>Country</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>15.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10th century</td>\n      <td>Harlem Globetrotters</td>\n      <td>2</td>\n      <td>4.5</td>\n      <td>0.707107</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>75.000000</td>\n      <td>24.041631</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_articles = article_combinations.copy()\n",
    "decoded_articles[['first_article', 'last_article']] = article_combinations[['first_article', 'last_article']].apply(lambda col: col.apply(decode_word))\n",
    "decoded_articles.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "(28718, 9)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_articles.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:22.462859500Z",
     "start_time": "2023-12-20T18:48:22.417858700Z"
    }
   },
   "id": "175f4c20d9574c89"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'Banknote'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_articles['last_article'][2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:22.493861Z",
     "start_time": "2023-12-20T18:48:22.433871800Z"
    }
   },
   "id": "dd9e8fde10522d52"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "\n",
    "# def apply_machine_both(index, row) -> list:\n",
    "#     source = row['first_article']\n",
    "#     target = row['last_article']\n",
    "# \n",
    "#     lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "#     lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "# \n",
    "#     result_dict[index] = [index, source, target, len(lib_explore_1)-1, lib_path_1, lib_explore_1, len(lib_explore_2)-1, lib_path_2, lib_explore_2]\n",
    "# \n",
    "#     return [index, source, target, len(lib_explore_1)-1, lib_path_1, lib_explore_1, len(lib_explore_2)-1, lib_path_2, lib_explore_2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:22.528859900Z",
     "start_time": "2023-12-20T18:48:22.450860600Z"
    }
   },
   "id": "ecd98a0aa7d7abba"
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "def apply_machine_first(graph, index, row) -> list:\n",
    "    source = row['first_article']\n",
    "    target = row['last_article']\n",
    "\n",
    "    lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(graph, source, target, heuristic=semantic_similarity)\n",
    "\n",
    "    return [index, source, target, len(lib_explore_1)-1, lib_path_1, lib_explore_1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T19:16:31.401084200Z",
     "start_time": "2023-12-20T19:16:31.369085500Z"
    }
   },
   "id": "fce6fef7bddbe481"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def apply_machine_second(index, row) -> list:\n",
    "    source = row['first_article']\n",
    "    target = row['last_article']\n",
    "\n",
    "    #lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "    lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "\n",
    "    return [index, source, target, len(lib_explore_2)-1, lib_path_2, lib_explore_2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:22.604858900Z",
     "start_time": "2023-12-20T18:48:22.483862300Z"
    }
   },
   "id": "4e667e7115253702"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "import concurrent\n",
    "import temp_workers\n",
    "import importlib\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "importlib.reload(temp_workers)\n",
    "\n",
    "temp_workers.set_graph(decoded_wikispeedia)\n",
    "temp_workers.set_similarity(semantic_similarity)\n",
    "\n",
    "total_cores = os.cpu_count()\n",
    "\n",
    "dummy_df = decoded_articles.iloc[12000:12002]\n",
    "\n",
    "# paths = Parallel(n_jobs=total_cores)(\n",
    "#     delayed(apply_machine_first)(item) for _, item in dummy_df.iterrows()\n",
    "# )\n",
    "\n",
    "\n",
    "# executor = concurrent.futures.ProcessPoolExecutor(max_workers=1)\n",
    "# futures = [executor.submit(temp_workers.apply_machine_both, item, semantic_similarity, decoded_wikispeedia) for _, item in dummy_df.iterrows()]\n",
    "# concurrent.futures.wait(futures)\n",
    "#print(paths)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:22.616866Z",
     "start_time": "2023-12-20T18:48:22.498860700Z"
    }
   },
   "id": "fc076e85c109b707"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28718, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": "      first_article last_article  count  mean_length  std_length  mean_rating  \\\n2318       Asteroid       Viking   1043     7.516779    3.019205     2.554455   \n4427          Brain    Telephone   1040     7.100000    3.580183     2.482051   \n25700       Theatre        Zebra    905     7.836464    3.849462     2.635359   \n21255       Pyramid         Bean    642     8.246106    4.259726     2.677419   \n3206         Batman         Wood    148     7.263514    3.413382     2.851852   \n\n       std_rating  mean_durationInSec  std_durationInSec  \n2318     0.980963          202.216683         281.960852  \n4427     1.043248          174.138462         189.381553  \n25700    1.044531          251.068508        1169.866464  \n21255    1.010459          199.085670         191.480244  \n3206     1.199478          113.466216         141.024406  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>first_article</th>\n      <th>last_article</th>\n      <th>count</th>\n      <th>mean_length</th>\n      <th>std_length</th>\n      <th>mean_rating</th>\n      <th>std_rating</th>\n      <th>mean_durationInSec</th>\n      <th>std_durationInSec</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2318</th>\n      <td>Asteroid</td>\n      <td>Viking</td>\n      <td>1043</td>\n      <td>7.516779</td>\n      <td>3.019205</td>\n      <td>2.554455</td>\n      <td>0.980963</td>\n      <td>202.216683</td>\n      <td>281.960852</td>\n    </tr>\n    <tr>\n      <th>4427</th>\n      <td>Brain</td>\n      <td>Telephone</td>\n      <td>1040</td>\n      <td>7.100000</td>\n      <td>3.580183</td>\n      <td>2.482051</td>\n      <td>1.043248</td>\n      <td>174.138462</td>\n      <td>189.381553</td>\n    </tr>\n    <tr>\n      <th>25700</th>\n      <td>Theatre</td>\n      <td>Zebra</td>\n      <td>905</td>\n      <td>7.836464</td>\n      <td>3.849462</td>\n      <td>2.635359</td>\n      <td>1.044531</td>\n      <td>251.068508</td>\n      <td>1169.866464</td>\n    </tr>\n    <tr>\n      <th>21255</th>\n      <td>Pyramid</td>\n      <td>Bean</td>\n      <td>642</td>\n      <td>8.246106</td>\n      <td>4.259726</td>\n      <td>2.677419</td>\n      <td>1.010459</td>\n      <td>199.085670</td>\n      <td>191.480244</td>\n    </tr>\n    <tr>\n      <th>3206</th>\n      <td>Batman</td>\n      <td>Wood</td>\n      <td>148</td>\n      <td>7.263514</td>\n      <td>3.413382</td>\n      <td>2.851852</td>\n      <td>1.199478</td>\n      <td>113.466216</td>\n      <td>141.024406</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does the code actually provide a speedup?\n",
    "# Let's check it out!\n",
    "# I'll run 12 and 12, to compare same size\n",
    "\n",
    "dummy_df = decoded_articles.copy(deep=True)\n",
    "\n",
    "dummy_df.sort_values(by='count', ascending=False, inplace=True)\n",
    "\n",
    "print(dummy_df.shape)\n",
    "dummy_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:22.629876400Z",
     "start_time": "2023-12-20T18:48:22.528859900Z"
    }
   },
   "id": "ff3854484f6198e7"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3860, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "      Unnamed: 0    first_article          last_article  path_count\n0              0         Asteroid                Viking        1043\n1              1            Brain             Telephone        1040\n2              2          Theatre                 Zebra         905\n3              3          Pyramid                  Bean         642\n4              4           Batman                  Wood         148\n...          ...              ...                   ...         ...\n3855        3855              Tin  Political philosophy           3\n3856        3856  Carcinus maenas                Riyadh           3\n3857        3857        Afrikaans          Invertebrate           3\n3858        3858         Botswana           Duran Duran           3\n3859        3859    North America               England           3\n\n[3860 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>first_article</th>\n      <th>last_article</th>\n      <th>path_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Asteroid</td>\n      <td>Viking</td>\n      <td>1043</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Brain</td>\n      <td>Telephone</td>\n      <td>1040</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Theatre</td>\n      <td>Zebra</td>\n      <td>905</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Pyramid</td>\n      <td>Bean</td>\n      <td>642</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Batman</td>\n      <td>Wood</td>\n      <td>148</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3855</th>\n      <td>3855</td>\n      <td>Tin</td>\n      <td>Political philosophy</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3856</th>\n      <td>3856</td>\n      <td>Carcinus maenas</td>\n      <td>Riyadh</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3857</th>\n      <td>3857</td>\n      <td>Afrikaans</td>\n      <td>Invertebrate</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3858</th>\n      <td>3858</td>\n      <td>Botswana</td>\n      <td>Duran Duran</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3859</th>\n      <td>3859</td>\n      <td>North America</td>\n      <td>England</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>3860 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished_paths = pd.read_csv('../paths_sample.csv'\n",
    "                             #names=['first_article','last_article','path_count']\n",
    "                             )\n",
    "\n",
    "finished_paths['first_article'] = finished_paths['first_article'].apply(decode_word)\n",
    "finished_paths['last_article'] = finished_paths['last_article'].apply(decode_word)\n",
    "\n",
    "print(finished_paths.shape)\n",
    "finished_paths"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T19:16:40.240246900Z",
     "start_time": "2023-12-20T19:16:40.184248900Z"
    }
   },
   "id": "2fcf2b7fb1dbe2ae"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0 first_article       last_article  path_count\n0           0      Asteroid             Viking        1043\n1           1         Brain          Telephone        1040\n2           2       Theatre              Zebra         905\n3           3       Pyramid               Bean         642\n4           4        Batman               Wood         148\n5           5          Bird  Great white shark         138",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>first_article</th>\n      <th>last_article</th>\n      <th>path_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Asteroid</td>\n      <td>Viking</td>\n      <td>1043</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Brain</td>\n      <td>Telephone</td>\n      <td>1040</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>Theatre</td>\n      <td>Zebra</td>\n      <td>905</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Pyramid</td>\n      <td>Bean</td>\n      <td>642</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Batman</td>\n      <td>Wood</td>\n      <td>148</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>Bird</td>\n      <td>Great white shark</td>\n      <td>138</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_df = finished_paths.iloc[:6, :]\n",
    "dummy_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T19:20:24.222722100Z",
     "start_time": "2023-12-20T19:20:24.190289900Z"
    }
   },
   "id": "614a76fd59737e0a"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in time: 54046.64505767822\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "paths = Parallel(n_jobs=6)(\n",
    "    delayed(apply_machine_first)(decoded_wikispeedia, index, row) for index, row in finished_paths.iterrows()\n",
    ")\n",
    "end_time = time.time()\n",
    "print(\"Finished in time:\", end_time-start_time)\n",
    "\n",
    "temp_df = pd.DataFrame(paths)\n",
    "temp_df.to_csv('run_a_star.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T10:21:46.028124Z",
     "start_time": "2023-12-20T19:20:57.328790800Z"
    }
   },
   "id": "e113122bf695584d"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "print('Finished!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T10:21:46.050123800Z",
     "start_time": "2023-12-21T10:21:46.031123900Z"
    }
   },
   "id": "a529bff52c0b1787"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "1125.8333333333333"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((105/6)*(3860))/60"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T19:11:28.568172900Z",
     "start_time": "2023-12-20T19:11:28.539176200Z"
    }
   },
   "id": "76e9d2cdcb199eb9"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-25-6b7789faf0d4>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      7\u001B[0m     \u001B[0mtarget\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mrow\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'last_article'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 9\u001B[1;33m     \u001B[0mlib_path_1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlib_explore_1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmachine_searchers\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodded_astar_path\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mwikispeedia\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msource\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mheuristic\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0msemantic_similarity\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     10\u001B[0m     \u001B[1;31m#lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Programming\\ada-2023-project-amonavis\\machine_searchers.py\u001B[0m in \u001B[0;36mmodded_astar_path\u001B[1;34m(G, source, target, heuristic, weight)\u001B[0m\n\u001B[0;32m    234\u001B[0m                     \u001B[1;32mcontinue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    235\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 236\u001B[1;33m                 \u001B[0mh\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mheuristic\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mneighbor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    237\u001B[0m             \u001B[0menqueued\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mneighbor\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mncost\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    238\u001B[0m             \u001B[0mpush\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mqueue\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mncost\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mh\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnext\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mneighbor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mncost\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcurnode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-5-eb1bed8486a1>\u001B[0m in \u001B[0;36msemantic_similarity\u001B[1;34m(word1, word2)\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[1;31m# Function to calculate semantic similarity between two pieces of text\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0msemantic_similarity\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mword2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m     \u001B[0membedding1\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_embedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m     \u001B[0membedding2\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_embedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword2\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-5-eb1bed8486a1>\u001B[0m in \u001B[0;36mget_embedding\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;31m# Function to get embeddings using sentence transformer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mget_embedding\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencode\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconvert_to_tensor\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mTrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[1;31m# Function to perform L2 normalization on the embeddings\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py\u001B[0m in \u001B[0;36mencode\u001B[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001B[0m\n\u001B[0;32m    151\u001B[0m             \u001B[0mdevice\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_target_device\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    152\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 153\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    154\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    155\u001B[0m         \u001B[0mall_embeddings\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36mto\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1143\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_floating_point\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_complex\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_blocking\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1144\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1145\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconvert\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1146\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1147\u001B[0m     def register_full_backward_pre_hook(\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    796\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 797\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    798\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    799\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    796\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 797\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    798\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    799\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    796\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 797\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    798\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    799\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    796\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 797\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    798\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    799\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    796\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 797\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    798\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    799\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    796\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 797\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    798\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    799\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    796\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 797\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    798\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    799\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    795\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    796\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mchildren\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 797\u001B[1;33m             \u001B[0mmodule\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    798\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    799\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtensor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtensor_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_apply\u001B[1;34m(self, fn)\u001B[0m\n\u001B[0;32m    818\u001B[0m             \u001B[1;31m# `with torch.no_grad():`\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    819\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mno_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 820\u001B[1;33m                 \u001B[0mparam_applied\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    821\u001B[0m             \u001B[0mshould_use_set_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcompute_should_use_set_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mparam_applied\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    822\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mshould_use_set_data\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36mconvert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1141\u001B[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001B[0;32m   1142\u001B[0m                             non_blocking, memory_format=convert_to_format)\n\u001B[1;32m-> 1143\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_floating_point\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mis_complex\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32melse\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_blocking\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1144\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1145\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_apply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mconvert\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for index, row in finished_paths.iterrows():\n",
    "    source = row['first_article']\n",
    "    target = row['last_article']\n",
    "\n",
    "    lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "    #lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "\n",
    "    # Now storing the index, to easily know what's not stored\n",
    "    result_dict[index] = [index, source, target, len(lib_explore_1)-1, lib_path_1, lib_explore_1\n",
    "        #, len(lib_explore_2)-1, lib_path_2, lib_explore_2\n",
    "        ]\n",
    "\n",
    "    if index % 3 == 0:\n",
    "        end_time = time.time()\n",
    "        print(\"Just finished:\", index)\n",
    "        print(\"In time:\", end_time-start_time)\n",
    "        start_time = end_time"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T19:05:55.906542500Z",
     "start_time": "2023-12-20T19:05:46.541544300Z"
    }
   },
   "id": "446e5ce6bda4b208"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Don't run this code!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mException\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-15-6f56cba8418b>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mraise\u001B[0m \u001B[0mException\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Don't run this code!\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mException\u001B[0m: Don't run this code!"
     ]
    }
   ],
   "source": [
    "raise Exception(\"Don't run this code!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:23.921941Z",
     "start_time": "2023-12-20T18:48:22.559862100Z"
    }
   },
   "id": "99df417217acafeb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Starting first test!\")\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "\n",
    "current_time = now.strftime(\"%H:%M:%S\")\n",
    "print(\"Current Time =\", current_time)\n",
    "\n",
    "result_dict = {}\n",
    "\n",
    "def apply_machine_both(index, row) -> list:\n",
    "    print(\"Starting\", index)\n",
    "    result_dict[index] = ['ass']\n",
    "\n",
    "    source = row['first_article']\n",
    "    target = row['last_article']\n",
    "\n",
    "    lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "    lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "\n",
    "    result_dict[index] = [index, source, target, len(lib_explore_1)-1, lib_path_1, lib_explore_1, len(lib_explore_2)-1, lib_path_2, lib_explore_2]\n",
    "    \n",
    "    print(\"Finished:\", index)\n",
    "    #return [index, source, target, len(lib_explore_1)-1, lib_path_1, lib_explore_1, len(lib_explore_2)-1, lib_path_2, lib_explore_2]\n",
    "\n",
    "#rel_rows = dummy_df.iloc[934:2000]\n",
    "\n",
    "rel_rows = dummy_df.iloc[934:955]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# I'm doing -2 so I can still run stuff on my laptop...\n",
    "total_cores = os.cpu_count() - 4\n",
    "\n",
    "paths_parallel = Parallel(n_jobs=total_cores, backend='multiprocessing')(\n",
    "    delayed(apply_machine_both)(index, item) for index, item in dummy_df.iterrows()\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Finished first parallel code\")\n",
    "print(\"Finished in:\", end_time-start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.923940600Z"
    }
   },
   "id": "cce0e7f1e1a09981"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raise Exception(\"Don't run this code!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:23.928957300Z",
     "start_time": "2023-12-20T18:48:23.926942200Z"
    }
   },
   "id": "cecee649c419e4ac"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "with mp.Pool() as p:\n",
    "    pool_results = p.map(apply_machine_both, [(index, item) for index, item in dummy_df.iterrows()]) #changed inputs\n",
    "    print(pool_results)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:23.935941200Z",
     "start_time": "2023-12-20T18:48:23.929939800Z"
    }
   },
   "id": "c6226b713110de0f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resulting_df = pd.DataFrame.from_dict(result_dict, orient='index',\n",
    "                                      columns=['Index',\n",
    "                                               'Source', 'Target', 'Length_1', 'Path_1', 'Explored_1'\n",
    "                                          , 'Length_2','Path_2', 'Explored_2'\n",
    "                                               ]\n",
    "                                      )\n",
    "\n",
    "print(resulting_df.shape)\n",
    "resulting_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.931940500Z"
    }
   },
   "id": "8c9502bcbf2cde7c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "paths_parallel"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.934940800Z"
    }
   },
   "id": "f2a52c88ecc2eab2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "other_result_dict = {}\n",
    "for i in range(len(paths_parallel)):\n",
    "    other_result_dict[i] = paths_parallel[i]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.941943100Z"
    }
   },
   "id": "8e857a238509a644"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resulting_df = pd.DataFrame.from_dict(result_dict, orient='index',\n",
    "                                      columns=['Index',\n",
    "                                               'Source', 'Target', 'Length_1', 'Path_1', 'Explored_1'\n",
    "                                          , 'Length_2','Path_2', 'Explored_2'\n",
    "                                               ]\n",
    "                                      )\n",
    "\n",
    "print(resulting_df.shape)\n",
    "resulting_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.942942600Z"
    }
   },
   "id": "c997f097c8e72db2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "name = 'machine_data_runs_' + str(269) + '_' + str(392) + '.csv'\n",
    "resulting_df.to_csv(name, encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.944941800Z"
    }
   },
   "id": "7007806e95e2952e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_read = pd.read_csv('machine_data_runs.csv')\n",
    "test_read"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.945941900Z"
    }
   },
   "id": "29c816e07f96c351"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "raise Exception(\"Don't run the rest of this code\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.946939500Z"
    }
   },
   "id": "8528894185625ac8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.947939800Z"
    }
   },
   "id": "adec6bd918945732"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "importlib.reload(temp_workers)\n",
    "\n",
    "temp_workers.set_graph(decoded_wikispeedia)\n",
    "temp_workers.set_similarity(semantic_similarity)\n",
    "\n",
    "temp = [temp_workers.apply_machine_both(item, semantic_similarity, decoded_wikispeedia) for _, item in dummy_df.iterrows()]\n",
    "temp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.948952500Z"
    }
   },
   "id": "8bd768a9f8b077e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "temp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-20T18:48:23.976941800Z",
     "start_time": "2023-12-20T18:48:23.949942Z"
    }
   },
   "id": "49a9ed87300cb08e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test = [(ass, item) for ass, item in dummy_df.iterrows()]\n",
    "test[0][1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.950940400Z"
    }
   },
   "id": "3d24eb051970ba40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# garbage = decoded_articles[:25]\n",
    "# \n",
    "# start_time = time.time()\n",
    "# temp_df = garbage.apply(apply_machine_first, axis=1, result_type='expand')\n",
    "# end_time = time.time()\n",
    "# print(\"Finished first\")\n",
    "# print(\" Found it in:\", end_time-start_time)\n",
    "# \n",
    "# \n",
    "# temp_2_df = garbage.apply(apply_machine_second, axis=1, result_type='expand')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.951942400Z"
    }
   },
   "id": "98b6a9983086282d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "splits = [(0, 6000), (6000, 12000), (12000, 18000), (18000, 24000), (24000, 28718)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.952956Z"
    }
   },
   "id": "7e622c59b979dc2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399e0a7f204a4e0",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-20T18:48:23.953940500Z"
    }
   },
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "\n",
    "chosen_split = splits[0]\n",
    "\n",
    "to_process = decoded_articles[269: chosen_split[1]]\n",
    "\n",
    "for index, row in to_process.iterrows():\n",
    "    source = row['first_article']\n",
    "    target = row['last_article']\n",
    "    \n",
    "    lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "    lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "\n",
    "    # Now storing the index, to easily know what's not stored\n",
    "    result_dict[index] = [index, source, target, len(lib_explore_1)-1, lib_path_1, lib_explore_1, len(lib_explore_2)-1, lib_path_2, lib_explore_2]\n",
    "\n",
    "    if index % 3 == 0:\n",
    "        print(\"Just finished:\", index)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
