{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Why?\n",
    "Idea of this notebook is to give the template that will be run on how to get all of the machine data.\n",
    "\n",
    "Namely, finding the paths and explorations that the two models take. \n",
    "Ideally it'll be upgraded to work with different models too, but that's for later"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f521180547c8733"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "import data_readers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "# networkx\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "\n",
    "\n",
    "# For semantic similarity\n",
    "from urllib.parse import unquote\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Python functions in .py file to read data\n",
    "import machine_searchers\n",
    "\n",
    "import warnings\n",
    "from tqdm import TqdmWarning\n",
    "warnings.filterwarnings('ignore', category=TqdmWarning)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e5d6d9e704dcf5c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "finished_paths = pd.read_csv('../datasets/wikispeedia_paths-and-graph/paths_finished.tsv', sep='\\t', skiprows=15,\n",
    "                                 names=['hashedIpAddress', 'timestamp', \"durationInSec\", 'path', \"rating\"])\n",
    "finished_paths['first_article'] = finished_paths['path'].apply(lambda x: x.split(';')[0])\n",
    "finished_paths['last_article'] = finished_paths['path'].apply(lambda x: x.split(';')[-1])\n",
    "finished_paths['path_length'] = finished_paths['path'].apply(lambda x: len(x.split(';')))\n",
    "finished_paths['date'] = pd.to_datetime(finished_paths['timestamp'], unit='s')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c52d6a923bfe0dcb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# How many each pair of articles has been visited\n",
    "article_combinations_count = finished_paths.groupby(['first_article', 'last_article']).size().reset_index(name='count')\n",
    "\n",
    "# The mean and std of the path length for each pair of articles\n",
    "article_combinations_stats = finished_paths.groupby(['first_article', 'last_article'])['path_length'].agg(['mean', 'std']).reset_index()\n",
    "article_combinations_stats['std'] = article_combinations_stats['std'].fillna(0)\n",
    "article_combinations_stats.rename(columns={'mean': 'mean_length', 'std': 'std_length'}, inplace=True)\n",
    "\n",
    "# The mean and std of the rating for each pair of articles. \n",
    "# Note that mean and std may be nan if there are nan ratings. We purposely leave them as nan, as we don't want to fill them with 0s or 1s.\n",
    "# Depending on the application, we could change this in the future if neeeded.\n",
    "rating_combinations_stats_rating = finished_paths.groupby(['first_article', 'last_article'])['rating'].agg(['mean', 'std']).reset_index()\n",
    "#rating_combinations_stats_rating['std'] = rating_combinations_stats_rating['std'].fillna(0)\n",
    "mask = rating_combinations_stats_rating['mean'].notnull()\n",
    "rating_combinations_stats_rating.loc[mask, 'std'] = rating_combinations_stats_rating.loc[mask, 'std'].fillna(0)\n",
    "rating_combinations_stats_rating.rename(columns={'mean': 'mean_rating', 'std': 'std_rating'}, inplace=True)\n",
    "\n",
    "# The mean and std of the time for each pair of articles.\n",
    "rating_combinations_stats_time = finished_paths.groupby(['first_article', 'last_article'])['durationInSec'].agg(['mean', 'std']).reset_index()\n",
    "rating_combinations_stats_time['std'] = rating_combinations_stats_time['std'].fillna(0)\n",
    "rating_combinations_stats_time.rename(columns={'mean': 'mean_durationInSec', 'std': 'std_durationInSec'}, inplace=True)\n",
    "\n",
    "# Merging all the dataframes\n",
    "article_combinations = pd.merge(article_combinations_count, article_combinations_stats, on=['first_article', 'last_article'])\n",
    "article_combinations = pd.merge(article_combinations, rating_combinations_stats_rating, on=['first_article', 'last_article'])\n",
    "article_combinations = pd.merge(article_combinations, rating_combinations_stats_time, on=['first_article', 'last_article'])\n",
    "\n",
    "# The number of unique sources and targets\n",
    "unique_sources = finished_paths['first_article'].value_counts().reset_index()\n",
    "unique_targets = finished_paths['last_article'].value_counts().reset_index()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b4531dd977229a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "article_combinations.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "341cf1f617a9831f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We define the model outside the function (make sure to run this before using the function)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7eda54fad92a69d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import machine_searchers\n",
    "import time\n",
    "\n",
    "def modded_get_embedding(text: str):\n",
    "    temp_str = text.replace('_', ' ')\n",
    "    temp_str = unquote(temp_str)\n",
    "    inputs = tokenizer(temp_str, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "def distance_two_words(w1: str, w2: str):\n",
    "    \"\"\"Receives a string that was in the wikispeedia dataset, and transforms it as needed to work\n",
    "    with the berd embeddings.\"\"\"\n",
    "\n",
    "    embedding1 = modded_get_embedding(w1)\n",
    "    embedding2 = modded_get_embedding(w2)\n",
    "    similarity = cosine_similarity(embedding1.detach().numpy(), embedding2.detach().numpy())[0][0]\n",
    "    # Adding absolute, just in case it is needed\n",
    "    # Similarity is actually 1 - abs(similarity) + 1,\n",
    "    # As we want closer words to have a smaller distance\n",
    "    # The last plus one is to indicate that there would be an extra cost to exploring, as if not the system often\n",
    "    # thinks that there are nodes that have a distance of 0.5 or something like that\n",
    "    similarity = 1 - abs(similarity) + 1\n",
    "    # print(\"First word:\", w1, \". Second word:\", w2, \". GoodDistance:\", similarity)\n",
    "    return similarity"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ea421c1f59d0660a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wikispeedia= nx.read_edgelist('../datasets/wikispeedia_paths-and-graph/links.tsv',\n",
    "                              create_using=nx.DiGraph)\n",
    "\n",
    "start_time = time.time()\n",
    "lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(wikispeedia, 'Actor', 'Japan', heuristic=distance_two_words)\n",
    "end_time = time.time()\n",
    "\n",
    "# It's len - 1 because the target node is also included, and that node wasn't explored\n",
    "print(\"Using the modded a star that returns explored nodes:\")\n",
    "print(\" Found solution for Actor to Japan exploring the following number of nodes:\", len(lib_explore_1)-1)\n",
    "print(\" Found it in:\", end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(wikispeedia, 'Actor', 'Japan', heuristic=distance_two_words)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Using depth first only A star that returns explored nodes:\")\n",
    "print(\" Found solution for Actor to Japan exploring the following number of nodes:\", len(lib_explore_2)-1)\n",
    "print(\" Found it in:\", end_time-start_time)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33593d5dce82fe85"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for index, row in article_combinations.iterrows():\n",
    "    source = row['first_article']\n",
    "    target = row['last_article']\n",
    "\n",
    "    lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(wikispeedia, source, target, heuristic=distance_two_words)\n",
    "    lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(wikispeedia, source, target, heuristic=distance_two_words)\n",
    "    \n",
    "    result_dict[index] = [source, target, lib_path_1, lib_explore_1, lib_path_2, lib_explore_2]\n",
    "    \n",
    "    # Placeholder code for testing\n",
    "    # if index >= 3:\n",
    "    #     break"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4399e0a7f204a4e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resulting_df = pd.DataFrame.from_dict(result_dict, orient='index', \n",
    "                                      columns=['Source', 'Target', 'Path_1', 'Explored_1', 'Path_2', 'Explored_2'])\n",
    "\n",
    "resulting_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66ac1344d60b0c68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resulting_df.to_csv('machine_data_runs.csv', encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5455c898591c8390"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_read = pd.read_csv('machine_data_runs.csv')\n",
    "test_read"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "750dfc281c120dd6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
