{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f521180547c8733",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Why?\n",
    "Idea of this notebook is to give the template that will be run on how to get all of the machine data.\n",
    "\n",
    "Namely, finding the paths and explorations that the two models take. \n",
    "Ideally it'll be upgraded to work with different models too, but that's for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e5d6d9e704dcf5c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:44:41.629153800Z",
     "start_time": "2023-12-08T21:44:41.546151900Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "import data_readers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "# networkx\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "\n",
    "# For semantic similarity\n",
    "from urllib.parse import unquote\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Python functions in .py file to read data\n",
    "import machine_searchers\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "from tqdm import TqdmWarning\n",
    "warnings.filterwarnings('ignore', category=TqdmWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c52d6a923bfe0dcb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:44:41.762154Z",
     "start_time": "2023-12-08T21:44:41.558151800Z"
    }
   },
   "outputs": [],
   "source": [
    "finished_paths = pd.read_csv('../datasets/wikispeedia_paths-and-graph/paths_finished.tsv', sep='\\t', skiprows=15,\n",
    "                                 names=['hashedIpAddress', 'timestamp', \"durationInSec\", 'path', \"rating\"])\n",
    "finished_paths['first_article'] = finished_paths['path'].apply(lambda x: x.split(';')[0])\n",
    "finished_paths['last_article'] = finished_paths['path'].apply(lambda x: x.split(';')[-1])\n",
    "finished_paths['path_length'] = finished_paths['path'].apply(lambda x: len(x.split(';')))\n",
    "finished_paths['date'] = pd.to_datetime(finished_paths['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b4531dd977229a8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:44:41.935151Z",
     "start_time": "2023-12-08T21:44:41.770150600Z"
    }
   },
   "outputs": [],
   "source": [
    "# How many each pair of articles has been visited\n",
    "article_combinations_count = finished_paths.groupby(['first_article', 'last_article']).size().reset_index(name='count')\n",
    "\n",
    "# The mean and std of the path length for each pair of articles\n",
    "article_combinations_stats = finished_paths.groupby(['first_article', 'last_article'])['path_length'].agg(['mean', 'std']).reset_index()\n",
    "article_combinations_stats['std'] = article_combinations_stats['std'].fillna(0)\n",
    "article_combinations_stats.rename(columns={'mean': 'mean_length', 'std': 'std_length'}, inplace=True)\n",
    "\n",
    "# The mean and std of the rating for each pair of articles. \n",
    "# Note that mean and std may be nan if there are nan ratings. We purposely leave them as nan, as we don't want to fill them with 0s or 1s.\n",
    "# Depending on the application, we could change this in the future if neeeded.\n",
    "rating_combinations_stats_rating = finished_paths.groupby(['first_article', 'last_article'])['rating'].agg(['mean', 'std']).reset_index()\n",
    "#rating_combinations_stats_rating['std'] = rating_combinations_stats_rating['std'].fillna(0)\n",
    "mask = rating_combinations_stats_rating['mean'].notnull()\n",
    "rating_combinations_stats_rating.loc[mask, 'std'] = rating_combinations_stats_rating.loc[mask, 'std'].fillna(0)\n",
    "rating_combinations_stats_rating.rename(columns={'mean': 'mean_rating', 'std': 'std_rating'}, inplace=True)\n",
    "\n",
    "# The mean and std of the time for each pair of articles.\n",
    "rating_combinations_stats_time = finished_paths.groupby(['first_article', 'last_article'])['durationInSec'].agg(['mean', 'std']).reset_index()\n",
    "rating_combinations_stats_time['std'] = rating_combinations_stats_time['std'].fillna(0)\n",
    "rating_combinations_stats_time.rename(columns={'mean': 'mean_durationInSec', 'std': 'std_durationInSec'}, inplace=True)\n",
    "\n",
    "# Merging all the dataframes\n",
    "article_combinations = pd.merge(article_combinations_count, article_combinations_stats, on=['first_article', 'last_article'])\n",
    "article_combinations = pd.merge(article_combinations, rating_combinations_stats_rating, on=['first_article', 'last_article'])\n",
    "article_combinations = pd.merge(article_combinations, rating_combinations_stats_time, on=['first_article', 'last_article'])\n",
    "\n",
    "# The number of unique sources and targets\n",
    "unique_sources = finished_paths['first_article'].value_counts().reset_index()\n",
    "unique_targets = finished_paths['last_article'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "341cf1f617a9831f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:44:41.951152700Z",
     "start_time": "2023-12-08T21:44:41.943153100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                    first_article          last_article  count  mean_length  \\\n0  %E2%82%AC2_commemorative_coins             Irish_Sea      1          3.0   \n1                    10th_century          11th_century      3          2.0   \n2                    10th_century              Banknote      1          5.0   \n3                    10th_century               Country      1          3.0   \n4                    10th_century  Harlem_Globetrotters      2          4.5   \n\n   std_length  mean_rating  std_rating  mean_durationInSec  std_durationInSec  \n0    0.000000     1.000000    0.000000           15.000000           0.000000  \n1    0.000000     2.333333    2.309401            4.333333           1.527525  \n2    0.000000     3.000000    0.000000           48.000000           0.000000  \n3    0.000000     1.000000    0.000000           15.000000           0.000000  \n4    0.707107     2.000000    0.000000           75.000000          24.041631  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>first_article</th>\n      <th>last_article</th>\n      <th>count</th>\n      <th>mean_length</th>\n      <th>std_length</th>\n      <th>mean_rating</th>\n      <th>std_rating</th>\n      <th>mean_durationInSec</th>\n      <th>std_durationInSec</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>%E2%82%AC2_commemorative_coins</td>\n      <td>Irish_Sea</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>15.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10th_century</td>\n      <td>11th_century</td>\n      <td>3</td>\n      <td>2.0</td>\n      <td>0.000000</td>\n      <td>2.333333</td>\n      <td>2.309401</td>\n      <td>4.333333</td>\n      <td>1.527525</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10th_century</td>\n      <td>Banknote</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>48.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10th_century</td>\n      <td>Country</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>15.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10th_century</td>\n      <td>Harlem_Globetrotters</td>\n      <td>2</td>\n      <td>4.5</td>\n      <td>0.707107</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>75.000000</td>\n      <td>24.041631</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_combinations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f71bc131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T21:44:42.263151Z",
     "start_time": "2023-12-08T21:44:41.954152700Z"
    }
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get embeddings using sentence transformer\n",
    "def get_embedding(text):\n",
    "    return model.encode(text, convert_to_tensor=True)\n",
    "\n",
    "# Function to perform L2 normalization on the embeddings\n",
    "def l2_normalize(tensor):\n",
    "    return tensor / tensor.norm(p=2, dim=0, keepdim=True)\n",
    "\n",
    "# Function to calculate semantic similarity between two pieces of text\n",
    "def semantic_similarity(word1, word2):\n",
    "    embedding1 = get_embedding(word1)\n",
    "    embedding2 = get_embedding(word2)\n",
    "\n",
    "    # L2 normalization of the embeddings (to make sure, although embedding should already be normalized)\n",
    "    embedding1_normalized = l2_normalize(embedding1)\n",
    "    embedding2_normalized = l2_normalize(embedding2)\n",
    "\n",
    "    # Compute and return the similarity of normalized tensors\n",
    "    return torch.dot(embedding1_normalized, embedding2_normalized).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33593d5dce82fe85",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:44:43.398150900Z",
     "start_time": "2023-12-08T21:44:42.268151400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the modded a star that returns explored nodes:\n",
      " Found solution for Actor to Japan exploring the following number of nodes: 1\n",
      " Found it in: 0.0969994068145752\n",
      "Using depth first only A star that returns explored nodes:\n",
      " Found solution for Actor to Japan exploring the following number of nodes: 1\n",
      " Found it in: 0.09199929237365723\n"
     ]
    }
   ],
   "source": [
    "wikispeedia= nx.read_edgelist('../datasets/wikispeedia_paths-and-graph/links.tsv',\n",
    "                              create_using=nx.DiGraph)\n",
    "\n",
    "def decode_word(word):\n",
    "    word = word.replace('_', ' ')\n",
    "    return unquote(word)\n",
    "\n",
    "# Create a new graph with decoded node labels\n",
    "decoded_wikispeedia = nx.DiGraph()\n",
    "\n",
    "for node in wikispeedia.nodes():\n",
    "    decoded_node = decode_word(node)\n",
    "    decoded_wikispeedia.add_node(decoded_node)\n",
    "\n",
    "# Copy the edges from the original graph to the new graph with decoded node labels\n",
    "for edge in wikispeedia.edges():\n",
    "    decoded_edge = tuple(decode_word(node) for node in edge)\n",
    "    decoded_wikispeedia.add_edge(*decoded_edge)\n",
    "\n",
    "start_time = time.time()\n",
    "lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(wikispeedia, 'Actor', 'Japan', heuristic=semantic_similarity)\n",
    "end_time = time.time()\n",
    "\n",
    "# It's len - 1 because the target node is also included, and that node wasn't explored\n",
    "print(\"Using the modded a star that returns explored nodes:\")\n",
    "print(\" Found solution for Actor to Japan exploring the following number of nodes:\", len(lib_explore_1)-1)\n",
    "print(\" Found it in:\", end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(wikispeedia, 'Actor', 'Japan', heuristic=semantic_similarity)\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Using depth first only A star that returns explored nodes:\")\n",
    "print(\" Found solution for Actor to Japan exploring the following number of nodes:\", len(lib_explore_2)-1)\n",
    "print(\" Found it in:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2af62950",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-08T21:44:43.486151800Z",
     "start_time": "2023-12-08T21:44:43.400152700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "            first_article          last_article  count  mean_length  \\\n0  €2 commemorative coins             Irish Sea      1          3.0   \n1            10th century          11th century      3          2.0   \n2            10th century              Banknote      1          5.0   \n3            10th century               Country      1          3.0   \n4            10th century  Harlem Globetrotters      2          4.5   \n\n   std_length  mean_rating  std_rating  mean_durationInSec  std_durationInSec  \n0    0.000000     1.000000    0.000000           15.000000           0.000000  \n1    0.000000     2.333333    2.309401            4.333333           1.527525  \n2    0.000000     3.000000    0.000000           48.000000           0.000000  \n3    0.000000     1.000000    0.000000           15.000000           0.000000  \n4    0.707107     2.000000    0.000000           75.000000          24.041631  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>first_article</th>\n      <th>last_article</th>\n      <th>count</th>\n      <th>mean_length</th>\n      <th>std_length</th>\n      <th>mean_rating</th>\n      <th>std_rating</th>\n      <th>mean_durationInSec</th>\n      <th>std_durationInSec</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>€2 commemorative coins</td>\n      <td>Irish Sea</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>15.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10th century</td>\n      <td>11th century</td>\n      <td>3</td>\n      <td>2.0</td>\n      <td>0.000000</td>\n      <td>2.333333</td>\n      <td>2.309401</td>\n      <td>4.333333</td>\n      <td>1.527525</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10th century</td>\n      <td>Banknote</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>48.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10th century</td>\n      <td>Country</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>15.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10th century</td>\n      <td>Harlem Globetrotters</td>\n      <td>2</td>\n      <td>4.5</td>\n      <td>0.707107</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>75.000000</td>\n      <td>24.041631</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_articles = article_combinations.copy()\n",
    "decoded_articles[['first_article', 'last_article']] = article_combinations[['first_article', 'last_article']].apply(lambda col: col.apply(decode_word))\n",
    "decoded_articles.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(28718, 9)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_articles.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:44:43.501151400Z",
     "start_time": "2023-12-08T21:44:43.482153400Z"
    }
   },
   "id": "175f4c20d9574c89"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'Banknote'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_articles['last_article'][2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:44:43.569150900Z",
     "start_time": "2023-12-08T21:44:43.497151400Z"
    }
   },
   "id": "dd9e8fde10522d52"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def apply_machine_first(row) -> list:\n",
    "    source = row['first_article']\n",
    "    target = row['last_article']\n",
    "\n",
    "    lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "    #lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "\n",
    "    return [source, target, len(lib_explore_1)-1, lib_path_1, lib_explore_1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:45:56.528151400Z",
     "start_time": "2023-12-08T21:45:56.470151800Z"
    }
   },
   "id": "fce6fef7bddbe481"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def apply_machine_second(row) -> list:\n",
    "    source = row['first_article']\n",
    "    target = row['last_article']\n",
    "\n",
    "    #lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "    lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "\n",
    "    return [source, target, len(lib_explore_2)-1, lib_path_2, lib_explore_2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:45:56.528151400Z",
     "start_time": "2023-12-08T21:45:56.484152500Z"
    }
   },
   "id": "4e667e7115253702"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# garbage = decoded_articles[:25]\n",
    "# \n",
    "# start_time = time.time()\n",
    "# temp_df = garbage.apply(apply_machine_first, axis=1, result_type='expand')\n",
    "# end_time = time.time()\n",
    "# print(\"Finished first\")\n",
    "# print(\" Found it in:\", end_time-start_time)\n",
    "# \n",
    "# \n",
    "# temp_2_df = garbage.apply(apply_machine_second, axis=1, result_type='expand')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:45:56.529151800Z",
     "start_time": "2023-12-08T21:45:56.500152100Z"
    }
   },
   "id": "98b6a9983086282d"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "splits = [(0, 6000), (6000, 12000), (12000, 18000), (18000, 24000), (24000, 28718)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-08T21:45:56.557150500Z",
     "start_time": "2023-12-08T21:45:56.517151400Z"
    }
   },
   "id": "7e622c59b979dc2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399e0a7f204a4e0",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-08T21:54:41.336254600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node Irish Sea not reachable from €2 commemorative coins in depth first version\n",
      "Just finished: 0\n",
      "Node Banknote not reachable from 10th century in depth first version\n",
      "Node Country not reachable from 10th century in depth first version\n",
      "Just finished: 3\n",
      "Node Harlem Globetrotters not reachable from 10th century in depth first version\n",
      "Node History of democracy not reachable from 10th century in depth first version\n",
      "Node Marco Polo not reachable from 10th century in depth first version\n",
      "Just finished: 6\n",
      "Node Dimetrodon not reachable from 11th century in depth first version\n",
      "Just finished: 9\n",
      "Node Education in the United States not reachable from 11th century in depth first version\n",
      "Node Hurricane Alex (2004) not reachable from 11th century in depth first version\n",
      "Just finished: 12\n",
      "Node John Adams not reachable from 11th century in depth first version\n",
      "Node Lhasa not reachable from 11th century in depth first version\n",
      "Just finished: 15\n",
      "Node Plum not reachable from 11th century in depth first version\n",
      "Node Taiwan not reachable from 11th century in depth first version\n",
      "Node Warsaw not reachable from 11th century in depth first version\n",
      "Just finished: 18\n",
      "Node Belfast not reachable from 12th century in depth first version\n",
      "Node Flat Earth not reachable from 12th century in depth first version\n",
      "Just finished: 21\n",
      "Node Geography of Ireland not reachable from 12th century in depth first version\n",
      "Just finished: 24\n",
      "Node Katana not reachable from 12th century in depth first version\n",
      "Node Suleiman the Magnificent not reachable from 12th century in depth first version\n",
      "Just finished: 27\n",
      "Node Zimbabwe not reachable from 12th century in depth first version\n",
      "Node Bison not reachable from 13th century in depth first version\n",
      "Just finished: 30\n",
      "Node Guernsey not reachable from 13th century in depth first version\n",
      "Node Lothal not reachable from 13th century in depth first version\n",
      "Node Neighbours not reachable from 13th century in depth first version\n",
      "Just finished: 33\n",
      "Node African slave trade not reachable from 14th century in depth first version\n",
      "Node Elizabeth I of England not reachable from 14th century in depth first version\n",
      "Just finished: 36\n",
      "Node Fire not reachable from 14th century in depth first version\n",
      "Node Henry David Thoreau not reachable from 14th century in depth first version\n",
      "Just finished: 39\n",
      "Node John F. Kennedy not reachable from 14th century in depth first version\n",
      "Node Rainbow not reachable from 14th century in depth first version\n"
     ]
    }
   ],
   "source": [
    "result_dict = {}\n",
    "\n",
    "chosen_split = splits[0]\n",
    "\n",
    "to_process = decoded_articles[chosen_split[0]: chosen_split[1]]\n",
    "\n",
    "for index, row in to_process.iterrows():\n",
    "    source = row['first_article']\n",
    "    target = row['last_article']\n",
    "    \n",
    "    lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "    lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(decoded_wikispeedia, source, target, heuristic=semantic_similarity)\n",
    "\n",
    "    # Now storing the index, to easily know what's not stored\n",
    "    result_dict[index] = [index, source, target, len(lib_explore_1)-1, lib_path_1, lib_explore_1, len(lib_explore_2)-1, lib_path_2, lib_explore_2]\n",
    "\n",
    "    if index % 3 == 0:\n",
    "        print(\"Just finished:\", index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66ac1344d60b0c68",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T10:35:55.284586Z",
     "start_time": "2023-12-09T10:35:55.131860500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(269, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": "     Index                  Source  \\\n0        0  €2 commemorative coins   \n1        1            10th century   \n2        2            10th century   \n3        3            10th century   \n4        4            10th century   \n..     ...                     ...   \n264    264                   4-6-0   \n265    265                   4-6-0   \n266    266                   4-6-0   \n267    267                   4-6-0   \n268    268                   4-6-0   \n\n                                                Target  Length_1  \\\n0                                            Irish Sea        44   \n1                                         11th century         1   \n2                                             Banknote        17   \n3                                              Country        21   \n4                                 Harlem Globetrotters       609   \n..                                                 ...       ...   \n264                                           Cataract      1484   \n265                                                Day       397   \n266                               Dwight D. Eisenhower       155   \n267                                      Julius Caesar       338   \n268  List of areas in the National Park System of t...         6   \n\n                                                Path_1  \\\n0    [€2 commemorative coins, United Kingdom, Irish...   \n1                         [10th century, 11th century]   \n2                   [10th century, Scotland, Banknote]   \n3                      [10th century, Mexico, Country]   \n4    [10th century, Poland, Pope John Paul II, Harl...   \n..                                                 ...   \n264  [4-6-0, Finland, Saint Petersburg, Leonhard Eu...   \n265                     [4-6-0, Petroleum, Earth, Day]   \n266  [4-6-0, Pennsylvania Railroad, PRR GG1, Dwight...   \n267            [4-6-0, Finland, France, Julius Caesar]   \n268  [4-6-0, United States, List of areas in the Na...   \n\n                                            Explored_1  Length_2  \\\n0    {'€2 commemorative coins': None, 'Nutrition': ...        98   \n1    {'10th century': None, '11th century': '10th c...         1   \n2    {'10th century': None, 'Black Sea': '10th cent...       306   \n3    {'10th century': None, 'Solar System': '10th c...       169   \n4    {'10th century': None, 'Algeria': '10th centur...       196   \n..                                                 ...       ...   \n264  {'4-6-0': None, 'Pennsylvania Railroad': '4-6-...       113   \n265  {'4-6-0': None, 'Pennsylvania Railroad': '4-6-...        72   \n266  {'4-6-0': None, '4-4-0': '4-6-0', 'Pennsylvani...       184   \n267  {'4-6-0': None, '4-4-0': '4-6-0', 'Pennsylvani...        61   \n268  {'4-6-0': None, '4-4-0': '4-6-0', 'Finland': '...       722   \n\n                                                Path_2  \\\n0                                                   []   \n1                         [10th century, 11th century]   \n2                                                   []   \n3                                                   []   \n4                                                   []   \n..                                                 ...   \n264                                                 []   \n265  [4-6-0, Pennsylvania Railroad, PRR GG1, Altern...   \n266  [4-6-0, 4-4-0, 2-8-0, 2-6-0, England, Sikhism,...   \n267                                                 []   \n268                                                 []   \n\n                                            Explored_2  \n0    {'€2 commemorative coins': None, 'Nutrition': ...  \n1    {'10th century': None, '11th century': '10th c...  \n2    {'10th century': None, 'Black Sea': '10th cent...  \n3    {'10th century': None, 'Solar System': '10th c...  \n4    {'10th century': None, 'Algeria': '10th centur...  \n..                                                 ...  \n264  {'4-6-0': None, 'Pennsylvania Railroad': '4-6-...  \n265  {'4-6-0': None, 'Pennsylvania Railroad': '4-6-...  \n266  {'4-6-0': None, '4-4-0': '4-6-0', '2-8-0': '4-...  \n267  {'4-6-0': None, '4-4-0': '4-6-0', '2-8-0': '4-...  \n268  {'4-6-0': None, '4-4-0': '4-6-0', '6-2-0': '4-...  \n\n[269 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Index</th>\n      <th>Source</th>\n      <th>Target</th>\n      <th>Length_1</th>\n      <th>Path_1</th>\n      <th>Explored_1</th>\n      <th>Length_2</th>\n      <th>Path_2</th>\n      <th>Explored_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>€2 commemorative coins</td>\n      <td>Irish Sea</td>\n      <td>44</td>\n      <td>[€2 commemorative coins, United Kingdom, Irish...</td>\n      <td>{'€2 commemorative coins': None, 'Nutrition': ...</td>\n      <td>98</td>\n      <td>[]</td>\n      <td>{'€2 commemorative coins': None, 'Nutrition': ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>10th century</td>\n      <td>11th century</td>\n      <td>1</td>\n      <td>[10th century, 11th century]</td>\n      <td>{'10th century': None, '11th century': '10th c...</td>\n      <td>1</td>\n      <td>[10th century, 11th century]</td>\n      <td>{'10th century': None, '11th century': '10th c...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>10th century</td>\n      <td>Banknote</td>\n      <td>17</td>\n      <td>[10th century, Scotland, Banknote]</td>\n      <td>{'10th century': None, 'Black Sea': '10th cent...</td>\n      <td>306</td>\n      <td>[]</td>\n      <td>{'10th century': None, 'Black Sea': '10th cent...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>10th century</td>\n      <td>Country</td>\n      <td>21</td>\n      <td>[10th century, Mexico, Country]</td>\n      <td>{'10th century': None, 'Solar System': '10th c...</td>\n      <td>169</td>\n      <td>[]</td>\n      <td>{'10th century': None, 'Solar System': '10th c...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>10th century</td>\n      <td>Harlem Globetrotters</td>\n      <td>609</td>\n      <td>[10th century, Poland, Pope John Paul II, Harl...</td>\n      <td>{'10th century': None, 'Algeria': '10th centur...</td>\n      <td>196</td>\n      <td>[]</td>\n      <td>{'10th century': None, 'Algeria': '10th centur...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>264</th>\n      <td>264</td>\n      <td>4-6-0</td>\n      <td>Cataract</td>\n      <td>1484</td>\n      <td>[4-6-0, Finland, Saint Petersburg, Leonhard Eu...</td>\n      <td>{'4-6-0': None, 'Pennsylvania Railroad': '4-6-...</td>\n      <td>113</td>\n      <td>[]</td>\n      <td>{'4-6-0': None, 'Pennsylvania Railroad': '4-6-...</td>\n    </tr>\n    <tr>\n      <th>265</th>\n      <td>265</td>\n      <td>4-6-0</td>\n      <td>Day</td>\n      <td>397</td>\n      <td>[4-6-0, Petroleum, Earth, Day]</td>\n      <td>{'4-6-0': None, 'Pennsylvania Railroad': '4-6-...</td>\n      <td>72</td>\n      <td>[4-6-0, Pennsylvania Railroad, PRR GG1, Altern...</td>\n      <td>{'4-6-0': None, 'Pennsylvania Railroad': '4-6-...</td>\n    </tr>\n    <tr>\n      <th>266</th>\n      <td>266</td>\n      <td>4-6-0</td>\n      <td>Dwight D. Eisenhower</td>\n      <td>155</td>\n      <td>[4-6-0, Pennsylvania Railroad, PRR GG1, Dwight...</td>\n      <td>{'4-6-0': None, '4-4-0': '4-6-0', 'Pennsylvani...</td>\n      <td>184</td>\n      <td>[4-6-0, 4-4-0, 2-8-0, 2-6-0, England, Sikhism,...</td>\n      <td>{'4-6-0': None, '4-4-0': '4-6-0', '2-8-0': '4-...</td>\n    </tr>\n    <tr>\n      <th>267</th>\n      <td>267</td>\n      <td>4-6-0</td>\n      <td>Julius Caesar</td>\n      <td>338</td>\n      <td>[4-6-0, Finland, France, Julius Caesar]</td>\n      <td>{'4-6-0': None, '4-4-0': '4-6-0', 'Pennsylvani...</td>\n      <td>61</td>\n      <td>[]</td>\n      <td>{'4-6-0': None, '4-4-0': '4-6-0', '2-8-0': '4-...</td>\n    </tr>\n    <tr>\n      <th>268</th>\n      <td>268</td>\n      <td>4-6-0</td>\n      <td>List of areas in the National Park System of t...</td>\n      <td>6</td>\n      <td>[4-6-0, United States, List of areas in the Na...</td>\n      <td>{'4-6-0': None, '4-4-0': '4-6-0', 'Finland': '...</td>\n      <td>722</td>\n      <td>[]</td>\n      <td>{'4-6-0': None, '4-4-0': '4-6-0', '6-2-0': '4-...</td>\n    </tr>\n  </tbody>\n</table>\n<p>269 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resulting_df = pd.DataFrame.from_dict(result_dict, orient='index', \n",
    "                                      columns=['Index','Source', 'Target', 'Length_1', 'Path_1', \n",
    "                                               'Explored_1', 'Length_2','Path_2', 'Explored_2'])\n",
    "\n",
    "print(resulting_df.shape)\n",
    "resulting_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5455c898591c8390",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-09T10:36:09.181646300Z",
     "start_time": "2023-12-09T10:36:08.631437300Z"
    }
   },
   "outputs": [],
   "source": [
    "name = 'machine_data_runs_' + str(chosen_split[0]) + '_' + str(269) + '.csv'\n",
    "resulting_df.to_csv(name, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750dfc281c120dd6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-08T21:53:53.674444400Z"
    }
   },
   "outputs": [],
   "source": [
    "test_read = pd.read_csv('machine_data_runs.csv')\n",
    "test_read"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
