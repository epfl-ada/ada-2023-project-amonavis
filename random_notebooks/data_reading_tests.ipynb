{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca19ec111240cf49",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Initial data exploration\n",
    "This is a simple initial data exploration created by Nick.\n",
    "\n",
    "The objective of this notebook in particular is the following:\n",
    "- Know how to read in the three different datasets that we have\n",
    "    - Might keep it only to the plain text and navigation paths, HTML might be too much\n",
    "- Find a way of linking the two datasets\n",
    "- Enrich the graph with the shortest path, or find a way of adding that info as well\n",
    "- Maybe see if there's a way of doing basic AF explorations already with networkX?\n",
    "\n",
    "Might also be worth it to create the environment we'll use for this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2767dbe5aa270d6",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T12:55:09.202279400Z",
     "start_time": "2023-11-17T12:55:06.258131200Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab5478e62b5531",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# How to use data readers\n",
    "Most of this notebook is just the template code that was used to make sure everything works.\n",
    "\n",
    "The important part is the following code block, that shows how to read in each of the different datasets that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c93a9b9f961f79b4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T12:55:17.561443800Z",
     "start_time": "2023-11-17T12:55:09.211474700Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'datasets/wikispeedia_paths-and-graph/links.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdata_readers\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# The links and edges\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m wikispeedia \u001B[38;5;241m=\u001B[39m data_readers\u001B[38;5;241m.\u001B[39mread_wikispeedia_graph()\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# The finished paths\u001B[39;00m\n\u001B[0;32m      7\u001B[0m finished_paths \u001B[38;5;241m=\u001B[39m data_readers\u001B[38;5;241m.\u001B[39mread_finished_paths()\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\ada-2023-project-amonavis\\data_readers.py:6\u001B[0m, in \u001B[0;36mread_wikispeedia_graph\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread_wikispeedia_graph\u001B[39m() \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m networkx\u001B[38;5;241m.\u001B[39mGraph:\n\u001B[1;32m----> 6\u001B[0m     wikispeedia \u001B[38;5;241m=\u001B[39m networkx\u001B[38;5;241m.\u001B[39mread_edgelist(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdatasets/wikispeedia_paths-and-graph/links.tsv\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m      7\u001B[0m                                          create_using\u001B[38;5;241m=\u001B[39mnetworkx\u001B[38;5;241m.\u001B[39mDiGraph)\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m wikispeedia\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\networkx\\utils\\decorators.py:766\u001B[0m, in \u001B[0;36margmap.__call__.<locals>.func\u001B[1;34m(_argmap__wrapper, *args, **kwargs)\u001B[0m\n\u001B[0;32m    765\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(\u001B[38;5;241m*\u001B[39margs, __wrapper\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 766\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m argmap\u001B[38;5;241m.\u001B[39m_lazy_compile(__wrapper)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m<class 'networkx.utils.decorators.argmap'> compilation 5:3\u001B[0m, in \u001B[0;36margmap_read_edgelist_1\u001B[1;34m(path, comments, delimiter, create_using, nodetype, data, edgetype, encoding)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbz2\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgzip\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01minspect\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\networkx\\utils\\decorators.py:189\u001B[0m, in \u001B[0;36mopen_file.<locals>._open_file\u001B[1;34m(path)\u001B[0m\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    186\u001B[0m     \u001B[38;5;66;03m# could be None, or a file handle, in which case the algorithm will deal with it\u001B[39;00m\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m path, \u001B[38;5;28;01mlambda\u001B[39;00m: \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m--> 189\u001B[0m fobj \u001B[38;5;241m=\u001B[39m _dispatch_dict[ext](path, mode\u001B[38;5;241m=\u001B[39mmode)\n\u001B[0;32m    190\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m fobj, \u001B[38;5;28;01mlambda\u001B[39;00m: fobj\u001B[38;5;241m.\u001B[39mclose()\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'datasets/wikispeedia_paths-and-graph/links.tsv'"
     ]
    }
   ],
   "source": [
    "import data_readers\n",
    "\n",
    "# The links and edges\n",
    "wikispeedia = data_readers.read_wikispeedia_graph()\n",
    "\n",
    "# The finished paths\n",
    "finished_paths = data_readers.read_finished_paths()\n",
    "\n",
    "# The unfinished paths\n",
    "unfinished_paths = data_readers.read_unfinished_paths()\n",
    "\n",
    "# The shortest path matrix\n",
    "# This one is the slowest to read by far, probably due to the weird parsing that has to be done!\n",
    "shortest_path_df = data_readers.read_shortest_path_df()\n",
    "\n",
    "# Searching for the string of a given article. It has to be formatted like the article name\n",
    "# Which shouldn't be a problem, as we'll probably usually retrieve them internally\n",
    "obi_wan_text = data_readers.plaintext_article_finder('Obi-Wan_Kenobi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shortest_path_df[('Actor',)][('Japan',)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-17T12:55:17.540263100Z"
    }
   },
   "id": "4f34aba44137af4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shortest_path_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-17T12:55:17.544069100Z"
    }
   },
   "id": "2fdda2af3747cfc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-17T12:55:17.546186200Z"
    }
   },
   "outputs": [],
   "source": [
    "shortest_path_df"
   ],
   "id": "a81d343284f5484c"
  },
  {
   "cell_type": "markdown",
   "id": "3d1013363bb6312c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There's four really important datasets in the wikispedia articles:\n",
    "- links.tsv: Contains the actual edges\n",
    "- paths_finished.tsv: Contains the winning games\n",
    "- paths_unfinished.tsv: Contains the losing games\n",
    "- shortest-path-distance-matrix.txt: Contains info on the shortest path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a77baa5f5d80e0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First part is checking that the reading of the data is correct. We know the number of edges and nodes in the dataset, so we'll just use that to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df634ad0a3b97aa",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-17T12:55:17.550951100Z"
    }
   },
   "outputs": [],
   "source": [
    "wikispeedia= networkx.read_edgelist('datasets/wikispeedia_paths-and-graph/links.tsv', \n",
    "                                    create_using=networkx.DiGraph)\n",
    "print(\"Dataset has\", len(wikispeedia.nodes), \"nodes\")\n",
    "print(\"Dataset has\", len(wikispeedia.edges), \"edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wikispeedia= networkx.read_edgelist('datasets/wikispeedia_paths-and-graph/links.tsv',\n",
    "                                    create_using=networkx.DiGraph)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-17T12:55:17.553356800Z"
    }
   },
   "id": "3d07213f45649804"
  },
  {
   "cell_type": "markdown",
   "id": "35b1076003cf6c53",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "These are less nodes than the reported number, it should be 4,604 nodes.\n",
    "\n",
    "The 119,882 edges is correct though.\n",
    "\n",
    "The difference is still small-ish, so for now I'll just ignore it and focus on reading in the other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02af7d11a7164a7",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-17T12:55:17.558138500Z"
    }
   },
   "outputs": [],
   "source": [
    "paths_finished = pd.read_csv('datasets/wikispeedia_paths-and-graph/paths_finished.tsv', sep='\\t', skiprows=15, \n",
    "                   names=['hashedIpAddress', 'timestamp', \"durationInSec\", 'path', \"rating\"])\n",
    "paths_finished.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9db75782ab34a37",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-17T12:55:17.563464800Z",
     "start_time": "2023-11-17T12:55:17.561443800Z"
    }
   },
   "outputs": [],
   "source": [
    "paths_unfinished = pd.read_csv('datasets/wikispeedia_paths-and-graph/paths_unfinished.tsv', sep='\\t', skiprows=16,\n",
    "                                names=['hashedIpAddress', 'timestamp', \"durationInSec\", 'path', \"target\", \"type\"])\n",
    "\n",
    "paths_unfinished.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227ccde75a1a6a7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Last part left is the shortest distance matrix. For this part, I just need to link things up with the articles.tsv to find the names corresponding to everything.\n",
    "\n",
    "Already reading the shortest distances is a bit of a pain though...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1447f7beb8d90a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-17T12:55:17.564463600Z"
    }
   },
   "outputs": [],
   "source": [
    "shortest_path = np.genfromtxt(\"datasets/wikispeedia_paths-and-graph/shortest-path-distance-matrix.txt\",\n",
    "                              delimiter=1, missing_values=-1, dtype=int)\n",
    "articles = pd.read_csv('datasets/wikispeedia_paths-and-graph/articles.tsv', sep='\\t', skiprows=12,\n",
    "                       names=[\"article_name\"])\n",
    "\n",
    "shortest_path_df = pd.DataFrame(shortest_path, index=articles.values, columns=articles.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T12:55:17.596297Z",
     "start_time": "2023-11-17T12:55:17.568362100Z"
    }
   },
   "outputs": [],
   "source": [
    "np.average(shortest_path_df)\n",
    "mask = [shortest_path_df == -1]\n",
    "# shortest_path_df[mask]\n",
    "\n",
    "shortest_path_df_temp = shortest_path_df.replace([-1, 0], pd.NA)\n",
    "\n",
    "# Calculate the average for each column\n",
    "average_values = shortest_path_df_temp.mean()\n",
    "average_values"
   ],
   "id": "1ed8f62a9c5ecb87"
  },
  {
   "cell_type": "markdown",
   "id": "bd6494ef20abf253",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Common file reader has already been created, I'll just have to do wait to transform things into a python file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbbfd0e6ef0d451",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Plain text reader\n",
    "Another basic AF reader, the objective of this is simply to find the relevant text file given the string name. Annoying because of the format the strings were given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452c3f550d2d2b31",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-17T12:55:17.570666600Z"
    }
   },
   "outputs": [],
   "source": [
    "text_file = open(\"datasets/plaintext_articles/%C3%85land.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "#read whole file to a string\n",
    "data = text_file.read()\n",
    "\n",
    "#close file\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d518a9efd883d4d",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-17T12:55:17.572711Z"
    }
   },
   "outputs": [],
   "source": [
    "def plaintext_article_finder(article_name: str) -> str:\n",
    "    art_file_name = \"datasets/plaintext_articles/\" + article_name + \".txt\"\n",
    "    text_file = open(art_file_name, \"r\", encoding=\"utf8\")\n",
    "    res_string = text_file.read()\n",
    "    text_file.close()\n",
    "    \n",
    "    return res_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert URL-encoded string to a regular Python string"
   ],
   "id": "1b2454f0d82d458"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-17T12:55:17.574937200Z"
    }
   },
   "outputs": [],
   "source": [
    "from urllib.parse import unquote\n",
    "\n",
    "# Decodes URL-encoded article names. \n",
    "# Changes to text with accents. For example, %C3%81ed%C3%A1n_mac_Gabr%C3%A1in becomes Áedán_mac_Gabráin.\n",
    "def decode_article(article_name):\n",
    "    encoded_string = (article_name)\n",
    "    decoded_string = unquote(encoded_string)\n",
    "    return decoded_string\n",
    "    \n",
    "decode_article(articles.iloc[0,0])"
   ],
   "id": "b1d052f592bf77d6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
