{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import random\n",
    "\n",
    "# networkx\n",
    "import networkx as nx\n",
    "\n",
    "# transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "\n",
    "# For semantic similarity\n",
    "from urllib.parse import unquote\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Python functions in .py file to read data\n",
    "import data_readers\n",
    "import machine_searchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The links and edges\n",
    "wikispeedia = data_readers.read_wikispeedia_graph()\n",
    "\n",
    "# The finished paths\n",
    "finished_paths = data_readers.read_finished_paths()\n",
    "\n",
    "# The unfinished paths\n",
    "unfinished_paths = data_readers.read_unfinished_paths()\n",
    "\n",
    "# DF of all articles\n",
    "articles = data_readers.read_articles()\n",
    "\n",
    "# DF of all articles and their categories\n",
    "categories = data_readers.read_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_finished_paths() -> pd.DataFrame:\n",
    "    paths_finished = pd.read_csv('paths_sample.csv', sep=',', \n",
    "                                 names=['first_article','last_article','path_count'])\n",
    "    return paths_finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_paths = read_finished_paths()\n",
    "unique_paths = finished_paths[['first_article', 'last_article']].drop_duplicates()\n",
    "sources = unique_paths['first_article']\n",
    "targets = unique_paths['last_article']\n",
    "unique_paths.sort_values(by=['first_article', 'last_article'], inplace=True)\n",
    "unique_paths.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(finished_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unique_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_paths_573 = unique_paths[573:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_paths_573"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset has\", len(wikispeedia.nodes), \"nodes (articles)\")\n",
    "print(\"Dataset has\", len(wikispeedia.edges), \"edges (links between articles)\")\n",
    "\n",
    "dic = nx.pagerank(wikispeedia)\n",
    "print(dic)\n",
    "\n",
    "for node in [nodo for nodo in wikispeedia.nodes()]:\n",
    "    wikispeedia.nodes[node]['pagerank'] = dic[node]\n",
    "print(wikispeedia.nodes(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get embeddings using sentence transformer\n",
    "def get_embedding(text):\n",
    "    return model.encode(text, convert_to_tensor=True)\n",
    "\n",
    "# Function to perform L2 normalization on the embeddings\n",
    "def l2_normalize(tensor):\n",
    "    return tensor / tensor.norm(p=2, dim=0, keepdim=True)\n",
    "\n",
    "# Function to calculate semantic similarity between two pieces of text\n",
    "def semantic_similarity(word1, word2):\n",
    "    embedding1 = get_embedding(word1)\n",
    "    embedding2 = get_embedding(word2)\n",
    "\n",
    "    # L2 normalization of the embeddings (to make sure, although embedding should already be normalized)\n",
    "    embedding1_normalized = l2_normalize(embedding1)\n",
    "    embedding2_normalized = l2_normalize(embedding2)\n",
    "\n",
    "    # Compute and return the similarity of normalized tensors\n",
    "    return torch.dot(embedding1_normalized, embedding2_normalized).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_similarity(\"Japan\",\"Asia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# si funciona, podemos hacer sample para mirar resultados de ir buscando cambiando la ref_similarity\n",
    "\n",
    "def degree_and_sem(G: nx.Graph, source: str, target: str, ref_similarity=0.3):\n",
    " # ref_similarity should be the avg sem dist\n",
    "\n",
    "   visited = set([])\n",
    "   current_children = []\n",
    "   sem_sim_childr = {}\n",
    "   max_page_childr = {}\n",
    "   path = []\n",
    "\n",
    "   current_node = source \n",
    "\n",
    "   found = False\n",
    "\n",
    "   while(not found):\n",
    "      visited.add(current_node)\n",
    "      path.append(current_node)\n",
    "      \n",
    "      if len(path) >= 25:\n",
    "         return source, target, found, len(path), path\n",
    "\n",
    "      if current_node == target:\n",
    "        found = True\n",
    "        return source, target, found, len(path), path,\n",
    "\n",
    "      current_children = list(G.successors(current_node))\n",
    "         \n",
    "      sem_sim_childr = {}\n",
    "      max_page_childr = {}\n",
    "      # store in a dic each child and its sem sim\n",
    "      for c in current_children:\n",
    "         if c == target:\n",
    "            found = True\n",
    "            visited.add(c)\n",
    "            path.append(c)\n",
    "            return source, target, found, len(path), path,\n",
    "         # compute semantic similarity\n",
    "         elif c in visited:\n",
    "            current_children.remove(c)\n",
    "         else:\n",
    "            semsim = semantic_similarity(c,target)\n",
    "            #store it with ID and sem sim\n",
    "            sem_sim_childr[c] = semsim\n",
    "            # compute semantic similarity\n",
    "            pagerank = G.nodes[c]['pagerank']\n",
    "            #store it with ID and sem sim\n",
    "            max_page_childr[c] = pagerank\n",
    "      if sem_sim_childr:\n",
    "         max_node = max(sem_sim_childr, key=sem_sim_childr.get)\n",
    "         max_sim = sem_sim_childr[max_node]\n",
    "         if max_sim >= ref_similarity:\n",
    "            #se va al de mayor similarity\n",
    "               current_node = max_node\n",
    "         else:\n",
    "            # se va al de mayor degree\n",
    "            max_node = max(max_page_childr, key=max_page_childr.get)\n",
    "            current_node = max_node\n",
    "      else:\n",
    "         current_children = list(G.successors(current_node))\n",
    "         current_node = random.choice(current_children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_and_sem(wikispeedia,'Boeing_747','Hawk-Eye')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('carol573.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['source',\t'target',\t'reached',\t'length',\t'visited']) \n",
    "\n",
    "    for index, row in unique_paths.iterrows():\n",
    "        source = row['first_article']\n",
    "        target = row['last_article']\n",
    "        \n",
    "        machine_result = degree_and_sem(wikispeedia, source, target)\n",
    "        writer.writerow(machine_result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('carol.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree_and_sem(wikispeedia,\"Quito\",\"Water\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machine_searchers.modded_astar_path(wikispeedia,\"Quito\",\"Water\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree_and_sem(wikispeedia,\"Switzerland\",\"Ant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machine_searchers.modded_astar_path(wikispeedia,\"Switzerland\",\"Ant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree_and_sem(wikispeedia,\"Space_Shuttle_Columbia_disaster\",\"Indus_Valley_Civilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machine_searchers.modded_astar_path(wikispeedia,\"Space_Shuttle_Columbia_disaster\",\"Indus_Valley_Civilization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree_and_sem(wikispeedia,\"Western_Roman_Empire\",\"Alcohol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#degree_and_sem(wikispeedia,'14th_century', 'Fire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#machine_searchers.modded_astar_path(wikispeedia,'14th_century', 'Fire')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
