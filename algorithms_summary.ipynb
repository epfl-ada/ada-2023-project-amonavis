{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Of the Different Algorithms Developed\n",
    "\n",
    "We have created different algorithms for a \"machine\" to emulate the behavior of humans, 3 in total.\n",
    "\n",
    "In this notebook we provide the code for each of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Useful Functions used Among All Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "from urllib.parse import unquote\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import time\n",
    "import warnings\n",
    "from tqdm import TqdmWarning\n",
    "warnings.filterwarnings('ignore', category=TqdmWarning)\n",
    "\n",
    "from data_readers import *\n",
    "\n",
    "import machine_searchers\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
    "\n",
    "# Function to get embeddings using sentence transformer\n",
    "def get_embedding(text):\n",
    "    return model.encode(text, convert_to_tensor=True)\n",
    "\n",
    "# Function to perform L2 normalization on the embeddings\n",
    "def l2_normalize(tensor):\n",
    "    return tensor / tensor.norm(p=2, dim=0, keepdim=True)\n",
    "\n",
    "# Function to calculate semantic similarity between two pieces of text\n",
    "def semantic_similarity(word1, word2):\n",
    "    embedding1 = get_embedding(word1)\n",
    "    embedding2 = get_embedding(word2)\n",
    "\n",
    "    # L2 normalization of the embeddings (to make sure, although embedding should already be normalized)\n",
    "    embedding1_normalized = l2_normalize(embedding1)\n",
    "    embedding2_normalized = l2_normalize(embedding2)\n",
    "\n",
    "    # Compute and return the similarity of normalized tensors\n",
    "    return torch.dot(embedding1_normalized, embedding2_normalized).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Links Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = read_wikispeedia_graph()\n",
    "pagerank = nx.pagerank(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Finished Paths File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first_article</th>\n",
       "      <th>last_article</th>\n",
       "      <th>path_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asteroid</td>\n",
       "      <td>Viking</td>\n",
       "      <td>1043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brain</td>\n",
       "      <td>Telephone</td>\n",
       "      <td>1040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Theatre</td>\n",
       "      <td>Zebra</td>\n",
       "      <td>905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pyramid</td>\n",
       "      <td>Bean</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Batman</td>\n",
       "      <td>Wood</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  first_article last_article  path_count\n",
       "0      Asteroid       Viking        1043\n",
       "1         Brain    Telephone        1040\n",
       "2       Theatre        Zebra         905\n",
       "3       Pyramid         Bean         642\n",
       "4        Batman         Wood         148"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finished_paths = read_finished_paths()\n",
    "unique_paths = finished_paths[['first_article', 'last_article']].drop_duplicates()\n",
    "sources = unique_paths['first_article']\n",
    "targets = unique_paths['last_article']\n",
    "unique_paths.sort_values(by=['first_article', 'last_article'], inplace=True)\n",
    "unique_paths.reset_index(inplace=True, drop=True)\n",
    "\n",
    "df = finished_paths[['first_article', 'last_article']].copy()\n",
    "df['path'] = df['first_article'] + '_' + df['last_article']\n",
    "df['path_count'] = df.groupby('path')['path'].transform('count')\n",
    "df.drop_duplicates(subset='path', inplace=True)\n",
    "df = df.sort_values('path_count', ascending = False)\n",
    "df = df[df['path_count']>2][['first_article', 'last_article', 'path_count']]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.to_csv('paths_sample.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Algorithm Carlos (DELETE THIS AND THINK OF A COOL NAME TO REFER TO IT) \n",
    "\n",
    "1. **Starting Point**\n",
    "   - Begin at the initial Wikipedia article.\n",
    "<br><br><br>\n",
    "\n",
    "2. **Evaluation of Links**\n",
    "   - Analyze links in the current article.\n",
    "<br><br><br>\n",
    "\n",
    "3. **PageRank and Topic Similarity Assessment**\n",
    "   - Prioritize links with the highest PageRank if topics of linked pages are not similar to the target article.\n",
    "   - Make a balance between PageRank and topical similarity if topics are somewhat similar.\n",
    "   - Increase focus on topical similarity as it becomes more similar to the target.\n",
    "<br><br><br>\n",
    "\n",
    "4. **Decision to Move Forward or Backward**\n",
    "   - Return to the previous page if all new links are less promising than the previous page (worse in PageRank and similarity).\n",
    "   - If not returning, avoid revisiting the same page to prevent loops.\n",
    "<br><br><br>\n",
    "\n",
    "5. **Limit on Page Visits**\n",
    "   - Stop if 20 different pages are visited without reaching the target article.\n",
    "<br><br><br>\n",
    "\n",
    "6. **End Goal**\n",
    "   - Continue the process until the target Wikipedia article is reached or the page visit limit is hit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(G, node_value, target_value):\n",
    "    \"\"\"\n",
    "    Calculate a value for a node based on its semantic similarity to the target and its PageRank.\n",
    "\n",
    "    Parameters:\n",
    "    G (networkx.Graph): The graph the node is part of.\n",
    "    node_value (str): The value of the current node.\n",
    "    target_value (str): The value of the target node.\n",
    "\n",
    "    Returns:\n",
    "    float: A calculated value for the node.\n",
    "    \"\"\"\n",
    "    # Calculate semantic similarity between the node and the target\n",
    "    similarity = semantic_similarity(node_value, target_value)\n",
    "\n",
    "    # Get PageRank of the node in graph G\n",
    "    node_pagerank = pagerank.get(node_value, None)\n",
    "\n",
    "    # Calculate the final value based on similarity and PageRank\n",
    "    if similarity < 0.1:\n",
    "        f = node_pagerank\n",
    "    elif 0.1 <= similarity <= 0.5:\n",
    "        f = similarity * node_pagerank\n",
    "    else:\n",
    "        f = similarity\n",
    "    return f\n",
    "\n",
    "def ai_1(graph, start_node, target_node):\n",
    "    \"\"\"\n",
    "    Algorithm to find a path in a graph from start_node to target_node.\n",
    "\n",
    "    Parameters:\n",
    "    graph (networkx.Graph): The graph to traverse.\n",
    "    start_node (str): The starting node in the graph.\n",
    "    target_node (str): The target node to reach in the graph.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the number of moves, the visited nodes list, and a flag indicating if the target was reached.\n",
    "    \"\"\"\n",
    "    # Initialize the starting node and visited nodes list\n",
    "    current_node = start_node\n",
    "    visited = []  # List to keep track of visited nodes\n",
    "    previous_node = start_node\n",
    "    reached_target = False\n",
    "    print(f\"Starting at node: {current_node}\")\n",
    "\n",
    "    # Iterate up to a maximum of 20 moves\n",
    "    for length in range(20):\n",
    "        # Check if the current node is the target\n",
    "        if current_node == target_node:\n",
    "            print(f\"Target node reached in {length} moves.\")\n",
    "            visited.append(previous_node)\n",
    "            visited.append(current_node)\n",
    "            reached_target = True\n",
    "            return length+1, visited, reached_target\n",
    "\n",
    "        # Mark the previous node as visited (except for the first move)\n",
    "        if length != 0:\n",
    "            visited.append(previous_node)\n",
    "        \n",
    "        # Update the previous node\n",
    "        previous_node = current_node\n",
    "\n",
    "        # Get unvisited neighbors of the current node\n",
    "        neighbors = list(graph.neighbors(current_node))\n",
    "        unvisited_neighbors = [n for n in neighbors if n not in visited and n != current_node]\n",
    "\n",
    "        # Choose the next node based on calculated value\n",
    "        if unvisited_neighbors:\n",
    "            next_node = max(unvisited_neighbors, key=lambda n: get_value(G, n, target_node))\n",
    "            current_node = next_node\n",
    "            print(f\"Moving to node: {current_node}\")\n",
    "        else:\n",
    "            # Exit if there are no unvisited neighbors\n",
    "            print(\"No more unvisited neighbors to move to.\")\n",
    "            return length+1, visited, reached_target\n",
    "\n",
    "    # If the loop exits due to reaching the move limit\n",
    "    print(\"Limit of 20 nodes reached.\")\n",
    "    return length+1, visited, reached_target\n",
    "\n",
    "# Note: The function `semantic_similarity` and the variable `pagerank` need to be defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at node: Zebra\n",
      "Moving to node: Animal\n",
      "Moving to node: Latin\n",
      "Moving to node: United_States\n",
      "Moving to node: Time_zone\n",
      "Moving to node: France\n",
      "Moving to node: List_of_countries_by_system_of_government\n",
      "Moving to node: People%27s_Republic_of_China\n",
      "Moving to node: English_language\n",
      "Moving to node: German_language\n",
      "Moving to node: United_Kingdom\n",
      "Moving to node: India\n",
      "Moving to node: Japan\n",
      "Moving to node: Vegetable\n",
      "Moving to node: Bean\n",
      "Target node reached in 14 moves.\n"
     ]
    }
   ],
   "source": [
    "ai_1(G, 'Zebra', 'Bean');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Algorithm Carol (DELETE THIS AND THINK OF A COOL NAME TO REFER TO IT) \n",
    "\n",
    "1. **Starting Point**\n",
    "   - Begin at the initial node in the network.\n",
    "   <br><br><br>\n",
    "\n",
    "2. **Exploring Connections**\n",
    "   - Examine the connections (successors) of the current node.\n",
    "   <br><br><br>\n",
    "\n",
    "3. **Semantic Similarity and PageRank Assessment**\n",
    "   - For each connection, calculate its semantic similarity to the target node.\n",
    "   - Obtain the PageRank for each connection.\n",
    "   - Store and compare these values for decision-making.\n",
    "   <br><br><br>\n",
    "\n",
    "4. **Choosing the Next Node**\n",
    "   - If a connection's semantic similarity is above a reference threshold, prioritize moving to the node with the highest similarity.\n",
    "   - Otherwise, move to the node with the highest PageRank.\n",
    "   - Skip nodes that have already been visited to prevent loops.\n",
    "   <br><br><br>\n",
    "\n",
    "5. **Random Selection as a Fallback**\n",
    "   - If there are no suitable nodes based on similarity or PageRank, choose a random successor to move to.\n",
    "   <br><br><br>\n",
    "\n",
    "6. **Limit on Node Visits**\n",
    "   - Terminate the process if 25 nodes are visited without reaching the target.\n",
    "   <br><br><br>\n",
    "\n",
    "7. **End Goal**\n",
    "   - Continue the process until the target node is reached or the node visit limit is hit.\n",
    "   <br><br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_2(G: nx.Graph, source: str, target: str, ref_similarity=0.3):\n",
    "   # Initialize visited nodes set, children lists, and path\n",
    "   visited = set([])\n",
    "   current_children = []\n",
    "   sem_sim_childr = {}\n",
    "   max_page_childr = {}\n",
    "   path = []\n",
    "\n",
    "   # Set the current node to the source\n",
    "   current_node = source \n",
    "\n",
    "   # Flag to check if target is found\n",
    "   found = False\n",
    "\n",
    "   print(f\"Starting at node: {current_node}\")\n",
    "\n",
    "   # Loop until the target is found or limit is reached\n",
    "   while not found:\n",
    "      # Mark the current node as visited and add to the path\n",
    "      visited.add(current_node)\n",
    "      path.append(current_node)\n",
    "   \n",
    "      # Check if the path length limit is reached\n",
    "      if len(path) >= 25:\n",
    "         print(\"Limit of 25 nodes reached.\")\n",
    "         return source, target, found, len(path), path\n",
    "\n",
    "      # Check if the target is reached\n",
    "      if current_node == target:\n",
    "         found = True\n",
    "         print(f\"Moving to node: {current_node}\")\n",
    "         print(f\"Target node reached in {len(path)} moves.\")\n",
    "         return source, target, found, len(path), path,\n",
    "\n",
    "      # Get the children (successors) of the current node\n",
    "      current_children = list(G.successors(current_node))\n",
    "      \n",
    "      # Reset the dictionaries for storing similarities and pageranks\n",
    "      sem_sim_childr = {}\n",
    "      max_page_childr = {}\n",
    "\n",
    "      # Iterate over children to calculate similarities and pageranks\n",
    "      for c in current_children:\n",
    "         # Check if the child is the target\n",
    "         if c == target:\n",
    "               found = True\n",
    "               visited.add(c)\n",
    "               path.append(c)\n",
    "               print(f\"Moving to node: {c}\")\n",
    "               print(f\"Target node reached in {len(path)} moves.\")\n",
    "               return source, target, found, len(path), path,\n",
    "\n",
    "         # Skip visited nodes\n",
    "         elif c in visited:\n",
    "               current_children.remove(c)\n",
    "         else:\n",
    "               # Compute semantic similarity\n",
    "               semsim = semantic_similarity(c, target)\n",
    "               sem_sim_childr[c] = semsim\n",
    "\n",
    "               # Compute pagerank\n",
    "               #pagerank = G.nodes[c]['pagerank']\n",
    "               max_page_childr[c] = pagerank.get(c, None)\n",
    "\n",
    "      # Choose the next node based on similarity or pagerank\n",
    "      if sem_sim_childr:\n",
    "         # Get the node with the maximum similarity\n",
    "         max_node = max(sem_sim_childr, key=sem_sim_childr.get)\n",
    "         max_sim = sem_sim_childr[max_node]\n",
    "         if max_sim >= ref_similarity:\n",
    "               # Move to the node with the highest similarity\n",
    "               current_node = max_node\n",
    "         else:\n",
    "               # Move to the node with the highest pagerank\n",
    "               max_node = max(max_page_childr, key=max_page_childr.get)\n",
    "               current_node = max_node\n",
    "      else:\n",
    "         # Choose a random successor if no suitable node is found\n",
    "         current_children = list(G.successors(current_node))\n",
    "         current_node = random.choice(current_children)\n",
    "   \n",
    "      print(f\"Moving to node: {current_node}\")\n",
    "\n",
    "# Note: The function `semantic_similarity` needs to be defined.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at node: Zebra\n",
      "Moving to node: Lion\n",
      "Moving to node: Tiger\n",
      "Moving to node: Bear\n",
      "Moving to node: Forest\n",
      "Moving to node: Tree\n",
      "Moving to node: Pea\n",
      "Moving to node: Bean\n",
      "Target node reached in 8 moves.\n"
     ]
    }
   ],
   "source": [
    "ai_2(G, 'Zebra', 'Bean');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Differences Between Algorithms 1 and 2\n",
    "\n",
    "Both algorithms navigate through networks (like Wikipedia articles) using PageRank and semantic similarity, but they have distinct approaches:\n",
    "\n",
    "### AI #1: The Wide Searcher\n",
    "- **Primary Focus**: Prioritizes PageRank initially, shifting towards semantic similarity as it becomes more relevant.\n",
    "- **Backtracking Mechanism**: Can backtrack to the previous node if new connections are less promising.\n",
    "- **Node Visit Limit**: Stops after exploring 20 nodes to prevent extensive wandering.\n",
    "- **Usage Scenario**: Gives more importance to the semantic similarity.\n",
    "\n",
    "### AI #2: The Dynamic Navigator\n",
    "- **Balanced Approach**: Dynamically balances between PageRank and semantic similarity from the start.\n",
    "- **Loop Prevention**: Avoids revisiting nodes to prevent loops, without backtracking.\n",
    "- **Extended Exploration**: Allows for exploration up to 25 nodes, providing a more extended search.\n",
    "- **Usage Scenario**: Explores nodes with high PageRank until the similarity is high.\n",
    "\n",
    "The key difference lies in their threshold for when to start focusing on similarity. AI #1 starts already looking at similarity for similarities > 0.1, while AI #2 starts looking at similarity when it is higher, >0.3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Algorithm Nico (DELETE THIS AND THINK OF A COOL NAME TO REFER TO IT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modded_get_embedding(text: str):\n",
    "    temp_str = text.replace('_', ' ')\n",
    "    temp_str = unquote(temp_str)\n",
    "    inputs = tokenizer(temp_str, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "def distance_two_words(w1: str, w2: str):\n",
    "    \"\"\"Receives a string that was in the wikispeedia dataset, and transforms it as needed to work\n",
    "    with the berd embeddings.\"\"\"\n",
    "\n",
    "    embedding1 = modded_get_embedding(w1)\n",
    "    embedding2 = modded_get_embedding(w2)\n",
    "    similarity = cosine_similarity(embedding1.detach().numpy(), embedding2.detach().numpy())[0][0]\n",
    "    # Adding absolute, just in case it is needed\n",
    "    # Similarity is actually 1 - abs(similarity) + 1,\n",
    "    # As we want closer words to have a smaller distance\n",
    "    # The last plus one is to indicate that there would be an extra cost to exploring, as if not the system often\n",
    "    # thinks that there are nodes that have a distance of 0.5 or something like that\n",
    "    similarity = 1 - abs(similarity) + 1\n",
    "    # print(\"First word:\", w1, \". Second word:\", w2, \". GoodDistance:\", similarity)\n",
    "    return similarity\n",
    "\n",
    "start_time = time.time()\n",
    "lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(wikispeedia, 'Actor', 'Japan', heuristic=distance_two_words)\n",
    "end_time = time.time()\n",
    "\n",
    "# It's len - 1 because the target node is also included, and that node wasn't explored\n",
    "print(\"Using the modded a star that returns explored nodes:\")\n",
    "print(\" Found solution for Actor to Japan exploring the following number of nodes:\", len(lib_explore_1)-1)\n",
    "print(\" Found it in:\", end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(wikispeedia, 'Actor', 'Japan', heuristic=distance_two_words)\n",
    "end_time = time.time()\n",
    "\n",
    "# It's len - 1 because the target node is also included, and that node wasn't explored\n",
    "print(\"Using depth first only A star that returns explored nodes:\")\n",
    "print(\" Found solution for Actor to Japan exploring the following number of nodes:\", len(lib_explore_1)-1)\n",
    "print(\" Found it in:\", end_time-start_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amonavis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
