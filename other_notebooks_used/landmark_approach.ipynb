{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Landmark approach\n",
    "Previous algorithms take hella long to run, we need simpler alternatives.\n",
    "\n",
    "First idea is the landmark approach.\n",
    "\n",
    "Gist of it is: Pick important nodes as landmarks, calculate shortest distances to and fro these for all other nodes. Just pick the ones with highest degree for this.\n",
    "\n",
    "Then when finding the shortest path, we just compare the landmarks, sum of the two values and pick the shortest path.\n",
    "\n",
    "Do another step of getting the path back, and we're golden"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266e210074bcf16a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "import data_readers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "# networkx\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "\n",
    "# For semantic similarity\n",
    "from urllib.parse import unquote\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Python functions in .py file to read data\n",
    "import machine_searchers\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "from tqdm import TqdmWarning\n",
    "warnings.filterwarnings('ignore', category=TqdmWarning)\n",
    "\n",
    "wikispeedia= nx.read_edgelist('../datasets/wikispeedia_paths-and-graph/links.tsv',\n",
    "                              create_using=nx.DiGraph)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71d68b146fe5700c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LandmarkSearch:\n",
    "    def __init__(self, graph: nx.DiGraph, landmark_num: int = 50):\n",
    "        # Default value should be a function of the size of the graph...\n",
    "        self.landmark_num = landmark_num\n",
    "\n",
    "        self.landmark_node_list = None\n",
    "        \n",
    "        # Empty dictionaries to store info\n",
    "        self.shortest_paths_to_node = {}\n",
    "        self.shortest_paths_from_node = {}\n",
    "        \n",
    "        self.fro_df = None\n",
    "        self.to_df = None\n",
    "        \n",
    "        self.get_landmark_info(graph, landmark_num)\n",
    "        \n",
    "    def get_landmark_info(self, graph: nx.DiGraph, landmark_num: int):\n",
    "        temp = sorted(graph.degree, key=lambda x: x[1], reverse=True)\n",
    "        temp = [elem[0] for elem in temp]\n",
    "        self.landmark_node_list = temp[:landmark_num]\n",
    "        \n",
    "        for elem in self.landmark_node_list:\n",
    "            self.shortest_paths_to_node[elem] = nx.single_target_shortest_path(graph, elem)\n",
    "            self.shortest_paths_from_node[elem] = nx.single_source_shortest_path(graph, elem)\n",
    "            \n",
    "        # Transforming the previous elements into a dict of lengths, because it's important\n",
    "        # But it's a dict of dicts!\n",
    "        paths_to_lengths = {}\n",
    "        paths_fro_lengths = {}\n",
    "\n",
    "        max_length = len(graph.nodes)\n",
    "\n",
    "        for elem in graph.nodes:\n",
    "            paths_fro_lengths[elem] = {}\n",
    "            paths_to_lengths[elem] = {}\n",
    "            for landmark in self.shortest_paths_from_node.keys():\n",
    "                # This extra code is to check if the key exists or not in the dictionaries\n",
    "                \n",
    "                # And fro and to are swapped, but that's because the dicts we save the info to\n",
    "                # are as well.\n",
    "                # So this ends up making sense\n",
    "                if elem in self.shortest_paths_from_node[landmark]:\n",
    "                    paths_to_lengths[elem][landmark] = len(self.shortest_paths_from_node[landmark][elem])\n",
    "                else:\n",
    "                    paths_to_lengths[elem][landmark] = max_length\n",
    "        \n",
    "                if elem in self.shortest_paths_to_node[landmark]:\n",
    "                    paths_fro_lengths[elem][landmark] = len(self.shortest_paths_to_node[landmark][elem])\n",
    "                else:\n",
    "                    paths_fro_lengths[elem][landmark] = max_length\n",
    "                    \n",
    "        # The easy way of distinguishing the two dfs is as follows:\n",
    "        # Get a loc[a, b]\n",
    "        # fro_df will describe distance from b to a\n",
    "        # to_df describes distance from a to b\n",
    "        self.fro_df = pd.DataFrame(paths_fro_lengths)\n",
    "        self.to_df = pd.DataFrame(paths_to_lengths)\n",
    "        \n",
    "    def find_shortest_path(self, source, target):\n",
    "        # For this, I sum up the two and fro somehow, and find the values!\n",
    "        temp_fro = self.fro_df.loc[:, source]\n",
    "        temp_to = self.to_df.loc[:, target]\n",
    "\n",
    "        distances = temp_to + temp_fro\n",
    "        distances.sort_values(inplace=True)\n",
    "        \n",
    "        landmark = distances.index[0]\n",
    "        \n",
    "        # The landmark is the middle point, this tells us the best one\n",
    "        start_path = self.shortest_paths_to_node[landmark][source][:-1]\n",
    "        end_path = self.shortest_paths_from_node[landmark][target]\n",
    "        \n",
    "        final_path = start_path + end_path\n",
    "        \n",
    "        return final_path"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7c6ecd8869a942fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "landmark_search = LandmarkSearch(wikispeedia, 2)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56576e0a9b72383"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "landmark_search.find_shortest_path('DVD', 'Compact_Disc')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0ad84dec15fe23c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay, it runs and the result makes some sense!\n",
    "\n",
    "We can also see in the example it's not perfect, as it can loop around. This is just a consequence of the way this was created. That's okay tbh!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e43bc4588adf8c48"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Double reading the paper, what they actually do is use the landmarks as a point of reference for A* search...\n",
    "\n",
    "Fuck it, it's a new idea. I'm also doing this because I'm pissed my other ideas aren't usable.\n",
    "\n",
    "Now, we'll do two things:\n",
    "- Number of landmarks is arbitrary, I'll just pick a fraction of the total number of nodes\n",
    "- Run the tests and get the data for the existing info\n",
    "\n",
    "There are better ways of picking landmarks, but this is good enough as a starting point. I can also segway this into the existing info, so fuck it."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "128bfbe56282ace5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def decode_word(word):\n",
    "    word = word.replace('_', ' ')\n",
    "    return unquote(word)\n",
    "\n",
    "# Create a new graph with decoded node labels\n",
    "decoded_wikispeedia = nx.DiGraph()\n",
    "\n",
    "for node in wikispeedia.nodes():\n",
    "    decoded_node = decode_word(node)\n",
    "    decoded_wikispeedia.add_node(decoded_node)\n",
    "\n",
    "# Copy the edges from the original graph to the new graph with decoded node labels\n",
    "for edge in wikispeedia.edges():\n",
    "    decoded_edge = tuple(decode_word(node) for node in edge)\n",
    "    decoded_wikispeedia.add_edge(*decoded_edge)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca341248f0d51224"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(decoded_wikispeedia.nodes())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "447defffd7ae5b35"
  },
  {
   "cell_type": "markdown",
   "source": [
    "I found no good explanation for taking more or less nodes... fuck it. I'll take 46, just because it's around 1\\%"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8122f9e8c37cd9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "landmark_search = LandmarkSearch(wikispeedia, 46)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "621b03cc14d6067b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reading in the nodes to explore\n",
    "finished_paths = pd.read_csv('../paths_sample.csv'\n",
    "                             #names=['first_article','last_article','path_count']\n",
    "                             )\n",
    "\n",
    "finished_paths.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e5f9fca198b0743"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_machine_landmark(row) -> list:\n",
    "    source = row['first_article']\n",
    "    target = row['last_article']\n",
    "\n",
    "    res = landmark_search.find_shortest_path(source, target)\n",
    "\n",
    "    return [source, target, len(res)-1, res]\n",
    "\n",
    "landmark_df = finished_paths.apply(apply_machine_landmark, axis=1, result_type='expand')\n",
    "landmark_df.columns = ['first_article','last_article','path_count', 'path']\n",
    "\n",
    "landmark_df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd1810b5216e13cb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "landmark_df['path'][0][0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7becee2ae57a9aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "landmark_df.to_csv('landmark_method_results.csv')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f23ba00649945466"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reading in the data properly is a pain. Writing this to figure out how to do it and get this organized!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "90972d1d772332ed"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "read_landmark = pd.read_csv('landmark_method_results.csv'\n",
    "                            #, converters={'path': lambda x: x[1:-1].split(\",\")}\n",
    "                )\n",
    "read_landmark['path'] = read_landmark['path'].str.strip('[]').str.split(',')\n",
    "#read_landmark['path'] = read_landmark['path'].apply(lambda x: x[1:-1])\n",
    "read_landmark.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4c3269eb9baf67"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "read_landmark['path'][0][0][1: -1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1506ec03e686315"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
