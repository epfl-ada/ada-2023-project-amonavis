{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GNN Model\n",
    "Original A* model takes too damn long to run. We are building a GNN to make up for it.\n",
    "\n",
    "The gist of how this one works is that it does a classification model to predict the likelihood of a node being part of the solution, as in normal classification. During actual prediction, it takes a moment to only add an element if it is valid in the current answer, IE it is connected to the existing nodes."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa510e1e01a5371c"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "import data_readers\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "# networkx\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "\n",
    "# For semantic similarity\n",
    "from urllib.parse import unquote\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Python functions in .py file to read data\n",
    "import machine_searchers\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "from tqdm import TqdmWarning\n",
    "warnings.filterwarnings('ignore', category=TqdmWarning)\n",
    "# I'll ignore the data embeddings, as that is "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:54:20.079679900Z",
     "start_time": "2023-12-19T15:54:08.925348Z"
    }
   },
   "id": "599c835735754e7"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "wikispeedia= nx.read_edgelist('../datasets/wikispeedia_paths-and-graph/links.tsv',\n",
    "                              create_using=nx.DiGraph)\n",
    "\n",
    "def decode_word(word):\n",
    "    word = word.replace('_', ' ')\n",
    "    return unquote(word)\n",
    "\n",
    "# Create a new graph with decoded node labels\n",
    "decoded_wikispeedia = nx.DiGraph()\n",
    "\n",
    "for node in wikispeedia.nodes():\n",
    "    decoded_node = decode_word(node)\n",
    "    decoded_wikispeedia.add_node(decoded_node)\n",
    "\n",
    "# Copy the edges from the original graph to the new graph with decoded node labels\n",
    "for edge in wikispeedia.edges():\n",
    "    decoded_edge = tuple(decode_word(node) for node in edge)\n",
    "    decoded_wikispeedia.add_edge(*decoded_edge)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:54:21.589793400Z",
     "start_time": "2023-12-19T15:54:20.084757Z"
    }
   },
   "id": "79c1b6d93c7fd0dc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building the embeddings\n",
    "There are a shit ton of ways of building the embeddings, for now, we will take a simpler approach of just using semantic distance. We also need to add to the embeddings if it's a source node and if it's a target node.\n",
    "\n",
    "For creating the training dataset, we also need to transform the shortest path pairs of the dataset into a vector of classifier. Which we need to test properly\n",
    "\n",
    "A decent amount of data creation..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5f548df91ad6d20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting all the embeddings\n",
    "\n",
    "We need to find the embeddings for each element. I'll just create the code and assume someone can do this better with cuda than what I feel like doing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2eb772bd121c3818"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "SentenceTransformer(\n  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n  (2): Normalize()\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do model to cuda here\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get embeddings using sentence transformer\n",
    "def get_embedding(text):\n",
    "    temp_embed = model.encode(text, convert_to_tensor=True)\n",
    "    temp_embed = temp_embed / temp_embed.norm(p=2, dim=0, keepdim=True)\n",
    "    return temp_embed\n",
    "\n",
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:54:23.390300500Z",
     "start_time": "2023-12-19T15:54:21.596824100Z"
    }
   },
   "id": "f37d201e1cb3f97b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([384])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = model.encode('Test', convert_to_tensor=True)\n",
    "temp.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:54:23.607666600Z",
     "start_time": "2023-12-19T15:54:23.393302200Z"
    }
   },
   "id": "fb6e8873e3b0a4d1"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m i \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m node_list:\n\u001B[1;32m----> 8\u001B[0m     text_embeddings[i] \u001B[38;5;241m=\u001B[39m get_embedding(node)\n\u001B[0;32m      9\u001B[0m     i \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[1;32mIn[3], line 6\u001B[0m, in \u001B[0;36mget_embedding\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_embedding\u001B[39m(text):\n\u001B[1;32m----> 6\u001B[0m     temp_embed \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mencode(text, convert_to_tensor\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      7\u001B[0m     temp_embed \u001B[38;5;241m=\u001B[39m temp_embed \u001B[38;5;241m/\u001B[39m temp_embed\u001B[38;5;241m.\u001B[39mnorm(p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m temp_embed\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:165\u001B[0m, in \u001B[0;36mSentenceTransformer.encode\u001B[1;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001B[0m\n\u001B[0;32m    162\u001B[0m features \u001B[38;5;241m=\u001B[39m batch_to_device(features, device)\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 165\u001B[0m     out_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(features)\n\u001B[0;32m    167\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m output_value \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_embeddings\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    168\u001B[0m         embeddings \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:66\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, features)\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m features:\n\u001B[0;32m     64\u001B[0m     trans_features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_type_ids\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m---> 66\u001B[0m output_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mtrans_features, return_dict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m     67\u001B[0m output_tokens \u001B[38;5;241m=\u001B[39m output_states[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     69\u001B[0m features\u001B[38;5;241m.\u001B[39mupdate({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtoken_embeddings\u001B[39m\u001B[38;5;124m'\u001B[39m: output_tokens, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m: features[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m'\u001B[39m]})\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1013\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m   1015\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[0;32m   1016\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m   1017\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1020\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[0;32m   1021\u001B[0m )\n\u001B[1;32m-> 1022\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(\n\u001B[0;32m   1023\u001B[0m     embedding_output,\n\u001B[0;32m   1024\u001B[0m     attention_mask\u001B[38;5;241m=\u001B[39mextended_attention_mask,\n\u001B[0;32m   1025\u001B[0m     head_mask\u001B[38;5;241m=\u001B[39mhead_mask,\n\u001B[0;32m   1026\u001B[0m     encoder_hidden_states\u001B[38;5;241m=\u001B[39mencoder_hidden_states,\n\u001B[0;32m   1027\u001B[0m     encoder_attention_mask\u001B[38;5;241m=\u001B[39mencoder_extended_attention_mask,\n\u001B[0;32m   1028\u001B[0m     past_key_values\u001B[38;5;241m=\u001B[39mpast_key_values,\n\u001B[0;32m   1029\u001B[0m     use_cache\u001B[38;5;241m=\u001B[39muse_cache,\n\u001B[0;32m   1030\u001B[0m     output_attentions\u001B[38;5;241m=\u001B[39moutput_attentions,\n\u001B[0;32m   1031\u001B[0m     output_hidden_states\u001B[38;5;241m=\u001B[39moutput_hidden_states,\n\u001B[0;32m   1032\u001B[0m     return_dict\u001B[38;5;241m=\u001B[39mreturn_dict,\n\u001B[0;32m   1033\u001B[0m )\n\u001B[0;32m   1034\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1035\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:612\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    603\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[0;32m    604\u001B[0m         create_custom_forward(layer_module),\n\u001B[0;32m    605\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    609\u001B[0m         encoder_attention_mask,\n\u001B[0;32m    610\u001B[0m     )\n\u001B[0;32m    611\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 612\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m layer_module(\n\u001B[0;32m    613\u001B[0m         hidden_states,\n\u001B[0;32m    614\u001B[0m         attention_mask,\n\u001B[0;32m    615\u001B[0m         layer_head_mask,\n\u001B[0;32m    616\u001B[0m         encoder_hidden_states,\n\u001B[0;32m    617\u001B[0m         encoder_attention_mask,\n\u001B[0;32m    618\u001B[0m         past_key_value,\n\u001B[0;32m    619\u001B[0m         output_attentions,\n\u001B[0;32m    620\u001B[0m     )\n\u001B[0;32m    622\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    623\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    536\u001B[0m     cross_attn_present_key_value \u001B[38;5;241m=\u001B[39m cross_attention_outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    537\u001B[0m     present_key_value \u001B[38;5;241m=\u001B[39m present_key_value \u001B[38;5;241m+\u001B[39m cross_attn_present_key_value\n\u001B[1;32m--> 539\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m apply_chunking_to_forward(\n\u001B[0;32m    540\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeed_forward_chunk, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchunk_size_feed_forward, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseq_len_dim, attention_output\n\u001B[0;32m    541\u001B[0m )\n\u001B[0;32m    542\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (layer_output,) \u001B[38;5;241m+\u001B[39m outputs\n\u001B[0;32m    544\u001B[0m \u001B[38;5;66;03m# if decoder, return the attn key/values as the last output\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:239\u001B[0m, in \u001B[0;36mapply_chunking_to_forward\u001B[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[0;32m    236\u001B[0m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[0;32m    237\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(output_chunks, dim\u001B[38;5;241m=\u001B[39mchunk_dim)\n\u001B[1;32m--> 239\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m forward_fn(\u001B[38;5;241m*\u001B[39minput_tensors)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:551\u001B[0m, in \u001B[0;36mBertLayer.feed_forward_chunk\u001B[1;34m(self, attention_output)\u001B[0m\n\u001B[0;32m    550\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeed_forward_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, attention_output):\n\u001B[1;32m--> 551\u001B[0m     intermediate_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintermediate(attention_output)\n\u001B[0;32m    552\u001B[0m     layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(intermediate_output, attention_output)\n\u001B[0;32m    553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:451\u001B[0m, in \u001B[0;36mBertIntermediate.forward\u001B[1;34m(self, hidden_states)\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m--> 451\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdense(hidden_states)\n\u001B[0;32m    452\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintermediate_act_fn(hidden_states)\n\u001B[0;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mlinear(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# Didn't run this part, but we just need to run it to be free\n",
    "node_list = decoded_wikispeedia.nodes()\n",
    "\n",
    "text_embeddings = torch.zeros((len(node_list), 384))\n",
    "\n",
    "i = 0\n",
    "for node in node_list:\n",
    "    text_embeddings[i] = get_embedding(node)\n",
    "    i += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:55:14.994097300Z",
     "start_time": "2023-12-19T15:54:23.594366900Z"
    }
   },
   "id": "1aebd067034d33f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.save(text_embeddings, 'text_embeddings.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:55:15.001846900Z",
     "start_time": "2023-12-19T15:55:14.996096700Z"
    }
   },
   "id": "434e83fc236d2666"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Getting the classification task\n",
    "\n",
    "For this, we run all shortest distance pairs to get the target classification module.\n",
    "\n",
    "We then transform this into a 2D tensor, with the following information:\n",
    "- First dimension is what is the source, and what is the target node. Everything else is 0\n",
    "- Second dimension has a 1 for all the nodes included in the shortest path. Including source and target\n",
    "\n",
    "Reason it's a 2D module is so we can extract the first one and add it to the embeddings, and the second is the actual target classification. It's just a way of guaranteeing all the units are together\n",
    "\n",
    "Also means the output will be a dictionary? I don't know if dictionary or a 3D tensor is better...\n",
    "\n",
    "I'll go with a dictionary because I don't feel like thinking. We can just pickle it or something"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "140b82168a07702e"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "all_shortest_paths = dict(nx.all_pairs_shortest_path(decoded_wikispeedia))\n",
    "shortest_paths_df = pd.DataFrame(all_shortest_paths)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:58:20.674584500Z",
     "start_time": "2023-12-19T15:55:18.674321400Z"
    }
   },
   "id": "abaaa78a24454db4"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "0    Áedán mac Gabráin\n1                 Bede\n2              Columba\n3            Dál Riata\n4        Great Britain\ndtype: object"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_of_elements = pd.Series(shortest_paths_df.index)\n",
    "series_of_elements.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T15:58:20.703217100Z",
     "start_time": "2023-12-19T15:58:20.680641200Z"
    }
   },
   "id": "e52ab24515e93b0b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 32\u001B[0m\n\u001B[0;32m     27\u001B[0m shortest_path_here \u001B[38;5;241m=\u001B[39m series_of_elements\u001B[38;5;241m.\u001B[39misin(shortest_paths_df\u001B[38;5;241m.\u001B[39mloc[target, source])\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# So each element in the dict is a tuple where the first element is the classification,\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# The second is the solution\u001B[39;00m\n\u001B[0;32m     31\u001B[0m shortest_paths_classification_dict[(source, target)] \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m---> 32\u001B[0m     torch\u001B[38;5;241m.\u001B[39mtensor(source_and_target, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32),\n\u001B[0;32m     33\u001B[0m     torch\u001B[38;5;241m.\u001B[39mtensor(shortest_path_here, dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[0;32m     34\u001B[0m )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# In the resulting DF the column is the source node\n",
    "# The target is the row\n",
    "\n",
    "# There's probably a pandas way of doing this that is nicer and not a bunch of shitty for loops\n",
    "# But that's okay\n",
    "shortest_paths_classification_dict = {}\n",
    "\n",
    "# I need a way of transforming an input into the classification part...\n",
    "# Having a pandas series with the values being the node names seems like the easiest solution!\n",
    "series_of_elements = pd.Series(shortest_paths_df.index)\n",
    "\n",
    "# IMPORTANT: This means the resulting nodes are in whatever is the current order\n",
    "# This can be shit, as the system might learn other stuff by coincidence...\n",
    "# Might also not matter?\n",
    "# We need a way of guaranteeing this order is kept!!!\n",
    "\n",
    "for source in node_list:\n",
    "    for target in node_list:\n",
    "        source_and_target = series_of_elements.isin([source, target])\n",
    "        # Order of target and source should make sense, it's just a consequence of how things are organized elsewhere\n",
    "        \n",
    "        # There are cases where the element is a nan, which is why we do this extra check\n",
    "        # This just skips the element, as there is no need to classify it then!\n",
    "        if type(shortest_paths_df.loc[target, source]) != list:\n",
    "            #print('No path between', source, target)\n",
    "            continue\n",
    "        shortest_path_here = series_of_elements.isin(shortest_paths_df.loc[target, source])\n",
    "        \n",
    "        # So each element in the dict is a tuple where the first element is the classification,\n",
    "        # The second is the solution\n",
    "        shortest_paths_classification_dict[(source, target)] = (\n",
    "            torch.tensor(source_and_target, dtype=torch.float32),\n",
    "            torch.tensor(shortest_path_here, dtype=torch.float32)\n",
    "        )\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:02:10.041116200Z",
     "start_time": "2023-12-19T15:58:20.707736100Z"
    }
   },
   "id": "15a6acd2f145d820"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "103356"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shortest_paths_classification_dict.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:02:36.899548900Z",
     "start_time": "2023-12-19T16:02:36.826276500Z"
    }
   },
   "id": "7bb47016cd0f9bcf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# At this point you should pickle the shortest_paths_classification_dict\n",
    "# this is to guarantee it works out!\n",
    "# Didn't write the code, but it's not ultra hard\n",
    "import pickle\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:02:10.043532900Z",
     "start_time": "2023-12-19T16:02:10.043532900Z"
    }
   },
   "id": "b0aacdd833113206"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([1., 0., 0.,  ..., 0., 0., 0.]),\n tensor([1., 1., 0.,  ..., 0., 0., 0.]))"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = shortest_paths_classification_dict[('Áedán mac Gabráin',\n",
    "                                    'German language')]\n",
    "\n",
    "temp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:02:39.306566100Z",
     "start_time": "2023-12-19T16:02:39.296450Z"
    }
   },
   "id": "444960a93908c8ef"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "<4592x4592 sparse array of type '<class 'numpy.intc'>'\n\twith 119882 stored elements in Compressed Sparse Row format>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Someone please double check that the orders line up!\n",
    "adj_matrix = nx.adjacency_matrix(decoded_wikispeedia, nodelist=node_list)\n",
    "adj_matrix"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:02:41.335955900Z",
     "start_time": "2023-12-19T16:02:41.235498400Z"
    }
   },
   "id": "ac63754c85faa201"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Building the problem generator\n",
    "All the parts are there, now is the part of kinda building the things together.\n",
    "\n",
    "Namely, this does the following:\n",
    "- Go over the shortest_paths_classification_dict and add the relevant source/target and the y value where needed\n",
    "- Checks the queries and only supplies problems that have not been done by humans. This is to make the testing more equal to avoid issues of dataleakage\n",
    "\n",
    "There is some extra smoothing that needs to be done here, particularly because there is already a dataloader class in pytorch and in pytorch geometric. But no fucking clue how to grab it!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d12993c615b18379"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# TODO: We really should store this article_combinations as a df... it's pretty important\n",
    "#   and calculating it all the time is a pain\n",
    "\n",
    "finished_paths = pd.read_csv('../datasets/wikispeedia_paths-and-graph/paths_finished.tsv', sep='\\t', skiprows=15,\n",
    "                             names=['hashedIpAddress', 'timestamp', \"durationInSec\", 'path', \"rating\"])\n",
    "finished_paths['first_article'] = finished_paths['path'].apply(lambda x: x.split(';')[0])\n",
    "finished_paths['last_article'] = finished_paths['path'].apply(lambda x: x.split(';')[-1])\n",
    "finished_paths['path_length'] = finished_paths['path'].apply(lambda x: len(x.split(';')))\n",
    "finished_paths['date'] = pd.to_datetime(finished_paths['timestamp'], unit='s')\n",
    "\n",
    "# How many each pair of articles has been visited\n",
    "article_combinations_count = finished_paths.groupby(['first_article', 'last_article']).size().reset_index(name='count')\n",
    "\n",
    "# The mean and std of the path length for each pair of articles\n",
    "article_combinations_stats = finished_paths.groupby(['first_article', 'last_article'])['path_length'].agg(['mean', 'std']).reset_index()\n",
    "article_combinations_stats['std'] = article_combinations_stats['std'].fillna(0)\n",
    "article_combinations_stats.rename(columns={'mean': 'mean_length', 'std': 'std_length'}, inplace=True)\n",
    "\n",
    "# The mean and std of the rating for each pair of articles. \n",
    "# Note that mean and std may be nan if there are nan ratings. We purposely leave them as nan, as we don't want to fill them with 0s or 1s.\n",
    "# Depending on the application, we could change this in the future if neeeded.\n",
    "rating_combinations_stats_rating = finished_paths.groupby(['first_article', 'last_article'])['rating'].agg(['mean', 'std']).reset_index()\n",
    "#rating_combinations_stats_rating['std'] = rating_combinations_stats_rating['std'].fillna(0)\n",
    "mask = rating_combinations_stats_rating['mean'].notnull()\n",
    "rating_combinations_stats_rating.loc[mask, 'std'] = rating_combinations_stats_rating.loc[mask, 'std'].fillna(0)\n",
    "rating_combinations_stats_rating.rename(columns={'mean': 'mean_rating', 'std': 'std_rating'}, inplace=True)\n",
    "\n",
    "# The mean and std of the time for each pair of articles.\n",
    "rating_combinations_stats_time = finished_paths.groupby(['first_article', 'last_article'])['durationInSec'].agg(['mean', 'std']).reset_index()\n",
    "rating_combinations_stats_time['std'] = rating_combinations_stats_time['std'].fillna(0)\n",
    "rating_combinations_stats_time.rename(columns={'mean': 'mean_durationInSec', 'std': 'std_durationInSec'}, inplace=True)\n",
    "\n",
    "# Merging all the dataframes\n",
    "article_combinations = pd.merge(article_combinations_count, article_combinations_stats, on=['first_article', 'last_article'])\n",
    "article_combinations = pd.merge(article_combinations, rating_combinations_stats_rating, on=['first_article', 'last_article'])\n",
    "article_combinations = pd.merge(article_combinations, rating_combinations_stats_time, on=['first_article', 'last_article'])\n",
    "\n",
    "article_combinations['first_article'] = article_combinations['first_article'].apply(decode_word)\n",
    "article_combinations['last_article'] = article_combinations['last_article'].apply(decode_word)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:02:45.402527300Z",
     "start_time": "2023-12-19T16:02:45.083478500Z"
    }
   },
   "id": "9ce45270040a3726"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "            first_article          last_article  count  mean_length  \\\n0  €2 commemorative coins             Irish Sea      1          3.0   \n1            10th century          11th century      3          2.0   \n2            10th century              Banknote      1          5.0   \n3            10th century               Country      1          3.0   \n4            10th century  Harlem Globetrotters      2          4.5   \n\n   std_length  mean_rating  std_rating  mean_durationInSec  std_durationInSec  \n0    0.000000     1.000000    0.000000           15.000000           0.000000  \n1    0.000000     2.333333    2.309401            4.333333           1.527525  \n2    0.000000     3.000000    0.000000           48.000000           0.000000  \n3    0.000000     1.000000    0.000000           15.000000           0.000000  \n4    0.707107     2.000000    0.000000           75.000000          24.041631  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>first_article</th>\n      <th>last_article</th>\n      <th>count</th>\n      <th>mean_length</th>\n      <th>std_length</th>\n      <th>mean_rating</th>\n      <th>std_rating</th>\n      <th>mean_durationInSec</th>\n      <th>std_durationInSec</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>€2 commemorative coins</td>\n      <td>Irish Sea</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>15.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10th century</td>\n      <td>11th century</td>\n      <td>3</td>\n      <td>2.0</td>\n      <td>0.000000</td>\n      <td>2.333333</td>\n      <td>2.309401</td>\n      <td>4.333333</td>\n      <td>1.527525</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10th century</td>\n      <td>Banknote</td>\n      <td>1</td>\n      <td>5.0</td>\n      <td>0.000000</td>\n      <td>3.000000</td>\n      <td>0.000000</td>\n      <td>48.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10th century</td>\n      <td>Country</td>\n      <td>1</td>\n      <td>3.0</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>15.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10th century</td>\n      <td>Harlem Globetrotters</td>\n      <td>2</td>\n      <td>4.5</td>\n      <td>0.707107</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>75.000000</td>\n      <td>24.041631</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_combinations.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:02:45.416854600Z",
     "start_time": "2023-12-19T16:02:45.403603200Z"
    }
   },
   "id": "66403b483e2eea5e"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "#result_list = list(zip(article_combinations['first_article'], article_combinations['last_article']))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:02:45.417855200Z",
     "start_time": "2023-12-19T16:02:45.414216400Z"
    }
   },
   "id": "4954fe8a0754c182"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "import random\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class ProblemGenerator:\n",
    "    def __init__(self, embedded_nodes: torch.tensor = None, classification_dict: dict = None,\n",
    "                 adjacency_matrix: torch.tensor = None, paths_not_to_use: pd.DataFrame = None,\n",
    "                 batch_size: int = 16):\n",
    "        if embedded_nodes is None:\n",
    "            # TODO: Add a way of reading in the pickled data of the tensors\n",
    "            # Remove the return, I just added it in to have some code here\n",
    "            pass\n",
    "        \n",
    "        if classification_dict is None:\n",
    "            # TODO: Pickle data here too\n",
    "            pass\n",
    "        \n",
    "        # TODO: Do the pickling thing also for the adjacency matrix and the paths not to use!\n",
    "        \n",
    "        self.embedded_nodes = embedded_nodes\n",
    "        self.classification_dict = classification_dict\n",
    "\n",
    "        # Transfomration of the adjacency matrix\n",
    "        values = adjacency_matrix.tocoo().data\n",
    "        indices = np.vstack((adjacency_matrix.tocoo().row, adjacency_matrix.tocoo().col))\n",
    "        \n",
    "        i = torch.LongTensor(indices)\n",
    "        v = torch.FloatTensor(values)\n",
    "        shape = adjacency_matrix.tocoo().shape\n",
    "\n",
    "        self.adjacency_matrix = torch.sparse_coo_tensor(i, v, torch.Size(shape))\n",
    "        \n",
    "        #self.adjacency_matrix = adjacency_matrix\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.remove_paths_not_to_use(paths_not_to_use)\n",
    "        \n",
    "    def remove_paths_not_to_use(self, path_not_to_use: pd.DataFrame):\n",
    "        \"\"\"Method goes over the classification dict, and removes any node pairs that\n",
    "        are contained in the paths not to use df\"\"\"\n",
    "        \n",
    "        # First part is creating a list that is just the first_article and last_article\n",
    "        \n",
    "        elems_to_remove = list(zip(article_combinations['first_article'], article_combinations['last_article']))\n",
    "        for elem in elems_to_remove:\n",
    "            self.classification_dict.pop(elem, None)\n",
    "        \n",
    "    def generate_problems_geometric(self, number_of_problems_to_generate: int = 200):\n",
    "        \"\"\"Generates problems.\n",
    "        \n",
    "        Namely, this means for the number_of_problems_to_generate, pick a problem,\n",
    "        attach the source and target matrix to the embeddings, and output the y value\n",
    "        as an adjacent value\n",
    "        \n",
    "        I guess this also means the output should be 3d, so we need to do some work on\n",
    "        the shape so that it has that structure!\n",
    "        \n",
    "        I'm not sure how to include the adjacency matrix in the output though, sorry :D\"\"\"\n",
    "        \n",
    "        # This should get the random keys\n",
    "        random_keys = random.sample(sorted(self.classification_dict.keys()), number_of_problems_to_generate)\n",
    "        \n",
    "        # Now, for each key generate a version of the problem\n",
    "        # I'll store the info in a 3d tensor, which I just update at each count!\n",
    "        # It's shape[1] because we add the source and target nodes to the inputs\n",
    "        emb_nodes_shape = self.embedded_nodes.shape\n",
    "        #X_values = torch.zeros((number_of_problems_to_generate, emb_nodes_shape[0], emb_nodes_shape[1] + 1))\n",
    "        \n",
    "        # This is a 2d matrix, first dim is the inputs\n",
    "        # second dim is the solutions for each of the relevant elems\n",
    "        #y_values = torch.zeros((number_of_problems_to_generate, emb_nodes_shape[0]))\n",
    "        data_elems = [None] * number_of_problems_to_generate\n",
    "        \n",
    "        i = 0\n",
    "        for elem in random_keys:\n",
    "            cur_source_and_target, cur_solution = self.classification_dict[elem]\n",
    "            \n",
    "            new_x = torch.cat([self.embedded_nodes, cur_source_and_target.unsqueeze(dim=1)], dim=1)\n",
    "\n",
    "            data_elems[i] = Data(x=new_x, edge_index=self.adjacency_matrix, y=cur_solution)\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "        return DataLoader(data_elems, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def generate_problems(self, number_of_problems_to_generate: int = 200):\n",
    "        \"\"\"Generates problems.\n",
    "        \n",
    "        Namely, this means for the number_of_problems_to_generate, pick a problem,\n",
    "        attach the source and target matrix to the embeddings, and output the y value\n",
    "        as an adjacent value\n",
    "        \n",
    "        I guess this also means the output should be 3d, so we need to do some work on\n",
    "        the shape so that it has that structure!\n",
    "        \n",
    "        I'm not sure how to include the adjacency matrix in the output though, sorry :D\"\"\"\n",
    "\n",
    "        # This should get the random keys\n",
    "        random_keys = random.sample(sorted(self.classification_dict.keys()), number_of_problems_to_generate)\n",
    "\n",
    "        # Now, for each key generate a version of the problem\n",
    "        # I'll store the info in a 3d tensor, which I just update at each count!\n",
    "        # It's shape[1] because we add the source and target nodes to the inputs\n",
    "        emb_nodes_shape = self.embedded_nodes.shape\n",
    "        X_values = torch.zeros((number_of_problems_to_generate, emb_nodes_shape[0], emb_nodes_shape[1] + 1))\n",
    "\n",
    "        # This is a 2d matrix, first dim is the inputs\n",
    "        # second dim is the solutions for each of the relevant elems\n",
    "        y_values = torch.zeros((number_of_problems_to_generate, emb_nodes_shape[0]))\n",
    "\n",
    "        i = 0\n",
    "        for elem in random_keys:\n",
    "            cur_source_and_target, cur_solution = self.classification_dict[elem]\n",
    "            y_values[i] = cur_solution\n",
    "\n",
    "            # TODO: I'm not sure if it's worth it to copy the embedded nodes...\n",
    "            # This is to avoid weird shit from happening with the gradient\n",
    "            X_values[i] = torch.cat([self.embedded_nodes, cur_source_and_target.unsqueeze(dim=1)], dim=1)\n",
    "            i += 1\n",
    "\n",
    "        return X_values, y_values\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:20:32.447088700Z",
     "start_time": "2023-12-19T16:20:32.442973100Z"
    }
   },
   "id": "2d792a297542495b"
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 4592, 385])\n",
      "torch.Size([5, 4592])\n"
     ]
    }
   ],
   "source": [
    "# This is to show what the structure is for the prob_gen!\n",
    "prob_gen = ProblemGenerator(text_embeddings, shortest_paths_classification_dict,\n",
    "                            adj_matrix, article_combinations)\n",
    "\n",
    "test_X, test_y = prob_gen.generate_problems(5)\n",
    "\n",
    "print(test_X.shape)\n",
    "print(test_y.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:20:34.204202900Z",
     "start_time": "2023-12-19T16:20:34.116716Z"
    }
   },
   "id": "5d129cdfc52253dd"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch_geometric.loader.dataloader.DataLoader at 0x205c8c1a5d0>"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = prob_gen.generate_problems_geometric(5)\n",
    "\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:20:39.609687500Z",
     "start_time": "2023-12-19T16:20:39.600457600Z"
    }
   },
   "id": "f5fefbeb8ee67360"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Okay, the previous parts work, which is good!\n",
    "\n",
    "They need to be added to separate files and we need to do some pickling, that's okay!"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edef34bfe3422653"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remark:\n",
    "\n",
    "After reading part of the documentation for pytorch geometric, we need to do some extra alterations for the inputs...\n",
    "\n",
    "Namely, I think it's related to the way we give the edge index. I haven't thought about this for too much, but it's important to note\n",
    "\n",
    "This should give an example of what to do. We have to adapt it of course, but at least it's an important part of\n",
    "what we need:\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/get_started/introduction.html"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2824ef9113fbe289"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "Data(x=[5, 4592, 385], edge_index=  (0, 1)\t1\n  (0, 2)\t1\n  (0, 3)\t1\n  (0, 4)\t1\n  (0, 5)\t1\n  (0, 6)\t1\n  (0, 7)\t1\n  (0, 8)\t1\n  (0, 9)\t1\n  (0, 10)\t1\n  (0, 11)\t1\n  (1, 4)\t1\n  (1, 61)\t1\n  (1, 182)\t1\n  (1, 261)\t1\n  (1, 565)\t1\n  (1, 630)\t1\n  (1, 634)\t1\n  (1, 637)\t1\n  (1, 1030)\t1\n  (1, 1053)\t1\n  (1, 1072)\t1\n  (1, 2838)\t1\n  (2, 3)\t1\n  (2, 5)\t1\n  :\t:\n  (4587, 348)\t1\n  (4587, 385)\t1\n  (4587, 466)\t1\n  (4587, 559)\t1\n  (4587, 1037)\t1\n  (4587, 1038)\t1\n  (4587, 1044)\t1\n  (4587, 1521)\t1\n  (4587, 1522)\t1\n  (4587, 1549)\t1\n  (4587, 1899)\t1\n  (4588, 699)\t1\n  (4588, 3665)\t1\n  (4589, 214)\t1\n  (4589, 3308)\t1\n  (4590, 309)\t1\n  (4590, 413)\t1\n  (4590, 572)\t1\n  (4590, 2392)\t1\n  (4591, 115)\t1\n  (4591, 183)\t1\n  (4591, 185)\t1\n  (4591, 273)\t1\n  (4591, 533)\t1\n  (4591, 1075)\t1, y=[5, 4592])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "\n",
    "#nx.generate_edgelist(decoded_wikispeedia)\n",
    "data = Data(x=test_X, edge_index=adj_matrix, y=test_y)\n",
    "\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:20:41.100690Z",
     "start_time": "2023-12-19T16:20:41.094245Z"
    }
   },
   "id": "80ebffcebbff869b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The actual GNN\n",
    "\n",
    "I'll keep it as basic as possible"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0ae5aa919bcf596"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, out_classes: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_features, 16)\n",
    "        self.conv2 = GCNConv(16, out_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:20:42.614755400Z",
     "start_time": "2023-12-19T16:20:42.609237400Z"
    }
   },
   "id": "22c99bd623f1dc3c"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch_geometric.loader.dataloader.DataLoader at 0x205c7d09190>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:08:07.555382200Z",
     "start_time": "2023-12-19T16:08:07.548778100Z"
    }
   },
   "id": "377af932e05ac884"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mAdamW(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m, weight_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5e-4\u001B[39m)\n\u001B[0;32m      8\u001B[0m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m----> 9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m data:\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(batch)\n\u001B[0;32m     11\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollate_fn(data)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\loader\\dataloader.py:55\u001B[0m, in \u001B[0;36mCollater.collate_fn\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, OnDiskDataset):\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39mmulti_get(batch))\n\u001B[1;32m---> 55\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(batch)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\loader\\dataloader.py:28\u001B[0m, in \u001B[0;36mCollater.__call__\u001B[1;34m(self, batch)\u001B[0m\n\u001B[0;32m     26\u001B[0m elem \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, BaseData):\n\u001B[1;32m---> 28\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m Batch\u001B[38;5;241m.\u001B[39mfrom_data_list(\n\u001B[0;32m     29\u001B[0m         batch,\n\u001B[0;32m     30\u001B[0m         follow_batch\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfollow_batch,\n\u001B[0;32m     31\u001B[0m         exclude_keys\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexclude_keys,\n\u001B[0;32m     32\u001B[0m     )\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(elem, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m default_collate(batch)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\batch.py:93\u001B[0m, in \u001B[0;36mBatch.from_data_list\u001B[1;34m(cls, data_list, follow_batch, exclude_keys)\u001B[0m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfrom_data_list\u001B[39m(\u001B[38;5;28mcls\u001B[39m, data_list: List[BaseData],\n\u001B[0;32m     83\u001B[0m                    follow_batch: Optional[List[\u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m     84\u001B[0m                    exclude_keys: Optional[List[\u001B[38;5;28mstr\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     85\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;124;03m    Python list of :class:`~torch_geometric.data.Data` or\u001B[39;00m\n\u001B[0;32m     87\u001B[0m \u001B[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[38;5;124;03m    :obj:`follow_batch`.\u001B[39;00m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 93\u001B[0m     batch, slice_dict, inc_dict \u001B[38;5;241m=\u001B[39m collate(\n\u001B[0;32m     94\u001B[0m         \u001B[38;5;28mcls\u001B[39m,\n\u001B[0;32m     95\u001B[0m         data_list\u001B[38;5;241m=\u001B[39mdata_list,\n\u001B[0;32m     96\u001B[0m         increment\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     97\u001B[0m         add_batch\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_list[\u001B[38;5;241m0\u001B[39m], Batch),\n\u001B[0;32m     98\u001B[0m         follow_batch\u001B[38;5;241m=\u001B[39mfollow_batch,\n\u001B[0;32m     99\u001B[0m         exclude_keys\u001B[38;5;241m=\u001B[39mexclude_keys,\n\u001B[0;32m    100\u001B[0m     )\n\u001B[0;32m    102\u001B[0m     batch\u001B[38;5;241m.\u001B[39m_num_graphs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(data_list)\n\u001B[0;32m    103\u001B[0m     batch\u001B[38;5;241m.\u001B[39m_slice_dict \u001B[38;5;241m=\u001B[39m slice_dict\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\collate.py:92\u001B[0m, in \u001B[0;36mcollate\u001B[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001B[0m\n\u001B[0;32m     89\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m# Collate attributes into a unified representation:\u001B[39;00m\n\u001B[1;32m---> 92\u001B[0m value, slices, incs \u001B[38;5;241m=\u001B[39m _collate(attr, values, data_list, stores,\n\u001B[0;32m     93\u001B[0m                                increment)\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, Tensor) \u001B[38;5;129;01mand\u001B[39;00m value\u001B[38;5;241m.\u001B[39mis_cuda:\n\u001B[0;32m     96\u001B[0m     device \u001B[38;5;241m=\u001B[39m value\u001B[38;5;241m.\u001B[39mdevice\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\data\\collate.py:197\u001B[0m, in \u001B[0;36m_collate\u001B[1;34m(key, values, data_list, stores, increment)\u001B[0m\n\u001B[0;32m    195\u001B[0m slices \u001B[38;5;241m=\u001B[39m cumsum(torch\u001B[38;5;241m.\u001B[39mtensor(repeats))\n\u001B[0;32m    196\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_torch_sparse_tensor(elem):\n\u001B[1;32m--> 197\u001B[0m     value \u001B[38;5;241m=\u001B[39m cat(values, dim\u001B[38;5;241m=\u001B[39mcat_dim)\n\u001B[0;32m    198\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    199\u001B[0m     value \u001B[38;5;241m=\u001B[39m torch_sparse\u001B[38;5;241m.\u001B[39mcat(values, dim\u001B[38;5;241m=\u001B[39mcat_dim)\n",
      "File \u001B[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\utils\\sparse.py:491\u001B[0m, in \u001B[0;36mcat\u001B[1;34m(tensors, dim)\u001B[0m\n\u001B[0;32m    488\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcat\u001B[39m(tensors: List[Tensor], dim: Union[\u001B[38;5;28mint\u001B[39m, Tuple[\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mint\u001B[39m]]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m    489\u001B[0m     \u001B[38;5;66;03m# TODO (matthias) We can make this more efficient by directly operating on\u001B[39;00m\n\u001B[0;32m    490\u001B[0m     \u001B[38;5;66;03m# the individual sparse tensor layouts.\u001B[39;00m\n\u001B[1;32m--> 491\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m dim \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, (\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)}\n\u001B[0;32m    493\u001B[0m     size \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    494\u001B[0m     edge_indices \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GCN(385, 2).to(device)\n",
    "\n",
    "data = prob_gen.generate_problems_geometric(5) #.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "model.train()\n",
    "for batch in data:\n",
    "    print(batch)\n",
    "    optimizer.zero_grad()\n",
    "    out = model(batch)\n",
    "    loss = F.nll_loss(out[batch.train_mask], data.y[batch.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:20:45.888887200Z",
     "start_time": "2023-12-19T16:20:45.397790500Z"
    }
   },
   "id": "bc69446708cc1e49"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got csr_array)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#torch.sparse()\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(batch\u001B[38;5;241m.\u001B[39medge_index[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;241m.\u001B[39mto_sparse()\n",
      "\u001B[1;31mTypeError\u001B[0m: expected np.ndarray (got csr_array)"
     ]
    }
   ],
   "source": [
    "#torch.sparse()\n",
    "torch.from_numpy(batch.edge_index[0]).to_sparse()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:15:30.030259500Z",
     "start_time": "2023-12-19T16:15:30.005078300Z"
    }
   },
   "id": "9022c252dde1609b"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "scipy.sparse._arrays.coo_array"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch.edge_index[0].tocoo())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:17:11.243641900Z",
     "start_time": "2023-12-19T16:17:11.238958900Z"
    }
   },
   "id": "6db1f7c14fe6af78"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(indices=tensor([[   0,    0,    0,  ..., 4591, 4591, 4591],\n                       [   1,    2,    3,  ...,  273,  533, 1075]]),\n       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n       size=(4592, 4592), nnz=119882, layout=torch.sparse_coo)"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:19:18.522444400Z",
     "start_time": "2023-12-19T16:19:18.517008400Z"
    }
   },
   "id": "4781fc9c1cc946fc"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(indices=tensor([[   0,    0,    0,  ..., 4591, 4591, 4591],\n                       [   1,    2,    3,  ...,  273,  533, 1075]]),\n       values=tensor([1., 1., 1.,  ..., 1., 1., 1.]),\n       size=(4592, 4592), nnz=119882, layout=torch.sparse_coo)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sparse_coo_tensor(i, v, torch.Size(shape))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-19T16:19:11.865223900Z",
     "start_time": "2023-12-19T16:19:11.858020500Z"
    }
   },
   "id": "9d4096603595f50d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
