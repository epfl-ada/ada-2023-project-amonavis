{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Deliverable 2\n",
    "This is the notebook that has the second deliverable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import modules\n",
    "Feel free to use the virtual environment (amonavis) included in the GitHub folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt \n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# For semantic similarity\n",
    "from urllib.parse import unquote\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Python functions in .py file to read data\n",
    "import data_readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Data reading\n",
    "The following code reads the data and saves them in the appropriate variables.\n",
    "<br><br>\n",
    "**Wikispeedia**\n",
    "This will hold our graph where Wikipedia articles are nodes and links/paths between them are edges.\n",
    "\n",
    "<br><br>\n",
    "**finished_paths**\n",
    "The datafile includes the hashedIpAddress, timestamp, durationInSec, path, and rating of games that were completed.\n",
    "We also add columns with the first article (soruce), last article (target), path length (#articles), and a readable date in Timestamp format.\n",
    "\n",
    "<br><br>\n",
    "**unfinished_paths**\n",
    "This datafile is similar to finished_paths, but with games that weren't completed. It has the hashedIpAddress, timestamp, durationInSec, path, target, and type of failure (either timeout or restart).\n",
    "\n",
    "<br><br>\n",
    "**articles**\n",
    "Dataframe with the name of all articles in the dataset.\n",
    "\n",
    "<br><br>\n",
    "**categories**\n",
    "This shows the higher category classification of each article. For example, Áedán mac Gabráin is part of 'subject.History.British_History.British_History_1500_and_before_including_Roman_Britain'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The links and edges\n",
    "wikispeedia = data_readers.read_wikispeedia_graph()\n",
    "\n",
    "# The finished paths\n",
    "finished_paths = data_readers.read_finished_paths()\n",
    "\n",
    "# The unfinished paths\n",
    "unfinished_paths = data_readers.read_unfinished_paths()\n",
    "\n",
    "# List of all articles\n",
    "articles = data_readers.read_articles()\n",
    "\n",
    "# List of all articles and their categories\n",
    "categories = data_readers.read_categories()\n",
    "\n",
    "\n",
    "# We found out later that the data contained in the shortest path matrix given to us seems to be wrong\n",
    "# Here we also add a quick dictionary that properly shows that this is wrong, and give an example\n",
    "shortest_path_df = data_readers.read_shortest_path_df()\n",
    "shortest_path_dict = dict(nx.all_pairs_shortest_path(wikispeedia))\n",
    "\n",
    "# Searching for the string of a given article. It has to be formatted like the article name\n",
    "# Which shouldn't be a problem, as we'll probably usually retrieve them internally\n",
    "obi_wan_text = data_readers.plaintext_article_finder('Obi-Wan_Kenobi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset has\", len(wikispeedia.nodes), \"nodes\")\n",
    "print(\"Dataset has\", len(wikispeedia.edges), \"edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "These are less nodes than the reported number, it should be 4,604 nodes.\n",
    "\n",
    "The 119,882 edges is correct though.\n",
    "\n",
    "The difference is probably due to some nodes not being connected to the rest of the graph, as here we read in only the articles that are connected. The few nodes that we are losing do not matter for what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print a sample of each datafram to make sure they were read in correctly.\n",
    "finished_paths.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfinished_paths.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_path_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Descriptive Data Analysis\n",
    "Here, we show that we understand what’s in the data (formats, distributions, missing values, correlations, etc.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section:\n",
    "<br>2.1. How many articles there are, how many paths \n",
    "<br>2.2. Histograms of the links from each article (for example, how many articles have 20 links, etc)\n",
    "<br>2.3. Average distance from one article to any other article\n",
    "<br>2.4. Histogram of the number of games at each point in time\n",
    "<br>2.5. Understanding unfinished games: Categories of targets in unfinished games\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. How many: articles, links, finished games, unfinished games?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are\", len(wikispeedia.nodes), \"articles in the dataset.\")\n",
    "print(\"There are\", len(wikispeedia.edges), \"links/paths.\")\n",
    "print(\"There are\", finished_paths.shape[0], \"finished games.\")\n",
    "print(\"There are\", unfinished_paths.shape[0], \"unfinished games.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_paths = finished_paths['path'].unique()\n",
    "print('There are', len(unique_paths), 'unique finshed paths.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Degree of a Node\n",
    "The degree of a node is the number of edges/links it has. We plot a complementary cumulative distribution function (CCDF) of degree for each article/node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the degrees of all nodes\n",
    "degrees = dict(wikispeedia.degree())\n",
    "\n",
    "# Calculate the CCDF\n",
    "degree_values = sorted(set(degrees.values()), reverse=True)\n",
    "ccdf = [sum(1 for degree in degrees.values() if degree >= d) for d in degree_values]\n",
    "\n",
    "# Plot the CCDF\n",
    "plt.plot(degree_values, ccdf, marker='o', linestyle='-', color='b')\n",
    "plt.xscale('log')  # Use log scale for better visualization\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Complementary Cumulative Distribution Function (CCDF)')\n",
    "plt.title('CCDF of Node Degrees')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the \"hubs\"? Which nodes have more than 500 links?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nodes with more than 1000 edges: \", [node for node in wikispeedia.nodes if wikispeedia.degree(node) >= 1000])\n",
    "print(\"Nodes with more than 500 edges: \", [node for node in wikispeedia.nodes if wikispeedia.degree(node) >= 500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that biggest hubs are mainly countries. The 'United Kingdom', 'France', 'United States', and 'Europe' have over 1000 links. Since there are 4592 nodes, these nodes have link to almost 1/4 of the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many nodes have more than 20 links? How many have only 1 link?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of nodes with degree 1\n",
    "nodes_degree_1 = [node for node in wikispeedia.nodes if wikispeedia.degree(node) == 1]\n",
    "print('Nodes with degree 1: ', nodes_degree_1)\n",
    "\n",
    "# Get the percentages\n",
    "total_nodes = len(wikispeedia.nodes)\n",
    "percentage_degree_1 = (len(nodes_degree_1) / total_nodes)\n",
    "print('% of nodes that have only 1 edge/link:', percentage_degree_1)\n",
    "\n",
    "# Count number of nodes with degree <= 20\n",
    "nodes_degree_20 = sum(1 for degree in degrees.values() if degree <= 20)\n",
    "print('% of nodes that have 20 or less edges/links:', nodes_degree_20 / total_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Average Distance between Articles\n",
    "On average, how many links/edges does it take to connect any random two articles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our graph is not strongly connected, meaning it's not possible to reach every node from every other node\n",
    "# So we can't use the built in function nx.average_shortest_path_length\n",
    "\n",
    "# This takes a long time to run (30 sec)\n",
    "shortest_paths = list(nx.all_pairs_shortest_path_length(wikispeedia))\n",
    "reachable_pairs = [(source, target, length) for source, paths in shortest_paths for target, length in paths.items() if length != float('inf')]\n",
    "total_distances = sum(length for _, _, length in reachable_pairs)\n",
    "average_distance = total_distances / len(reachable_pairs) if reachable_pairs else 0\n",
    "print(f\"Average distance between reachable nodes: {average_distance:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Games per Time\n",
    "Histogram of the number of games at each point in time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamps to datetime objects\n",
    "timestamps = finished_paths['timestamp']\n",
    "date_times = [datetime.utcfromtimestamp(ts) for ts in timestamps]\n",
    "\n",
    "# Create a histogram of timestamps\n",
    "plt.hist(date_times, bins=20, color='skyblue')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Timestamps')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of games were played in Q3 2009! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3_2009_times = [dt for dt in date_times if dt.year == 2009 and dt.month in [7, 8, 9]]\n",
    "percent_q3_2009_times = len(q3_2009_times) / len(date_times)\n",
    "print(f\"Percent of finished games completed in Q3 2009: {percent_q3_2009_times:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Which categories of games are more likely to be unfinished? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are', categories['article'].duplicated().sum(), 'articles with more than 1 category.')\n",
    "\n",
    "print('Should we drop the duplicate categories, or doublecount them?')\n",
    "print('This corresponds to', categories['article'].duplicated().sum() / len(wikispeedia.nodes), 'of the articles.')\n",
    "\n",
    "# Let's drop them for now.\n",
    "categories['article'] = categories['article'].drop_duplicates()\n",
    "print('The new shape is: ',categories.shape)\n",
    "\n",
    "# Why are there more articles here than nodes (# articles)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use string manipulation to extract the highest level category for each article.\n",
    "\n",
    "sub_categories = categories['categories'].str[8:].str.split('.')\n",
    "category_depth_1 = sub_categories.apply(lambda x: x[0])\n",
    "categories['depth_1'] = category_depth_1\n",
    "categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the category corresponding to each unfinished target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging categories with unfinished paths.\n",
    "unfinished_paths_with_categories = pd.merge(unfinished_paths, categories, left_on = 'target', right_on= 'article', how = 'left')\n",
    "\n",
    "# Count the occurrences of each category\n",
    "category_counts = unfinished_paths_with_categories['depth_1'].value_counts()\n",
    "\n",
    "# Plotting the bar chart\n",
    "plt.bar(category_counts.index, category_counts.values)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Depth 1 Categories')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency of Categories of Target Articles in Unfinished Paths')\n",
    "\n",
    "plt.xticks(rotation=90)  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()  # Ensure labels fit within the plot area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that targets in the science category make up the largest proportion of unfinished games. In Deliverable 3, we will investigate this more. We'll discover if this is because most of the articles are from the science category, or if science articles are actually harder to find in the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Issue with the shortest path\n",
    "The shortest paths that were calculated are wrong. This is just a simple example to show that at least one of the points is wrong. We assume this is common enough elsewhere, as the effort needed to properly prove this is too large.\n",
    "\n",
    "We double-checked the method for reading in the data, and there are no big changes to be done there. We also double-checked that the graph is a directed graph, which matters for this search. Additionally, we manually checked the edges, so we know that the following example is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We have to put the strings into tuples to make this work. It's ugly, but this works\n",
    "print(\"Shortest path from Actor to Japan in the given matrix is:\", shortest_path_df[('Actor',)][('Japan',)])\n",
    "\n",
    "print(\"Shortest path from Actor to Japan according to networkX is:\", len(shortest_path_dict['Actor']['Japan'])-1)\n",
    "\n",
    "print(\"The actual path is:\", shortest_path_dict['Actor']['Japan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to print the entire text, just show that reading it in works\n",
    "obi_wan_text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Study of Unique Paths\n",
    "Here we study the unique source and target pairs. We will use the dataframes to compare the performance between humans and machines, as well as to know what paths to make machines complete.\n",
    "\n",
    "**article_combinations**\n",
    "\n",
    "This dataframe contains information on all the combination of source and target articles in the finished games (paths). It includes how many times it has been played, and the mean and std of the path length, duration of the game, and rating.\n",
    "\n",
    "**unique_targets** and **unique_sources**\n",
    "\n",
    "These dataframes include all the sources and targets that appears in the finished games\n",
    "\n",
    "<br><br>\n",
    "Note that we don't change to ASCII the name of the articles yet. We will do it at a later step if we need to.\n",
    "<br><br>\n",
    "\n",
    "\n",
    "<div style=\"border: 2px solid white; padding: 10px;\">\n",
    "    <font color=\"red\">feel free to move this wherever works better. Maybe not the best thing to have at the beginning of the notebook.</font>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many each pair of articles has been visited\n",
    "article_combinations_count = finished_paths.groupby(['first_article', 'last_article']).size().reset_index(name='count')\n",
    "\n",
    "# The mean and std of the path length for each pair of articles\n",
    "article_combinations_stats = finished_paths.groupby(['first_article', 'last_article'])['path_length'].agg(['mean', 'std']).reset_index()\n",
    "article_combinations_stats['std'] = article_combinations_stats['std'].fillna(0)\n",
    "article_combinations_stats.rename(columns={'mean': 'mean_length', 'std': 'std_length'}, inplace=True)\n",
    "\n",
    "# The mean and std of the rating for each pair of articles. \n",
    "    # Note that mean and std may be nan if there are nan ratings. We purposely leave them as nan, as we don't want to fill them with 0s or 1s.\n",
    "    # Depending on the application, we could change this in the future if neeeded.\n",
    "rating_combinations_stats_rating = finished_paths.groupby(['first_article', 'last_article'])['rating'].agg(['mean', 'std']).reset_index()\n",
    "#rating_combinations_stats_rating['std'] = rating_combinations_stats_rating['std'].fillna(0)\n",
    "mask = rating_combinations_stats_rating['mean'].notnull()\n",
    "rating_combinations_stats_rating.loc[mask, 'std'] = rating_combinations_stats_rating.loc[mask, 'std'].fillna(0)\n",
    "rating_combinations_stats_rating.rename(columns={'mean': 'mean_rating', 'std': 'std_rating'}, inplace=True)\n",
    "\n",
    "# The mean and std of the time for each pair of articles.\n",
    "rating_combinations_stats_time = finished_paths.groupby(['first_article', 'last_article'])['durationInSec'].agg(['mean', 'std']).reset_index()\n",
    "rating_combinations_stats_time['std'] = rating_combinations_stats_time['std'].fillna(0)\n",
    "rating_combinations_stats_time.rename(columns={'mean': 'mean_durationInSec', 'std': 'std_durationInSec'}, inplace=True)\n",
    "\n",
    "# Merging all the dataframes\n",
    "article_combinations = pd.merge(article_combinations_count, article_combinations_stats, on=['first_article', 'last_article'])\n",
    "article_combinations = pd.merge(article_combinations, rating_combinations_stats_rating, on=['first_article', 'last_article'])\n",
    "article_combinations = pd.merge(article_combinations, rating_combinations_stats_time, on=['first_article', 'last_article'])\n",
    "\n",
    "# The number of unique sources and targets\n",
    "unique_sources = finished_paths['first_article'].value_counts().reset_index()\n",
    "unique_targets = finished_paths['last_article'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_combinations.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_sources.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_targets.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Semantic Similarity\n",
    "\n",
    "An important part of the project is to study how humans and machines move from article to article. Semantic similarity compares two strings based on a trained model and assigns a value according to how are they correlated (the higher, the more related). For example, 'king' and 'queen' will have a higher semantic similarity than say, 'king' and 'chemistry' (will prove this here)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the underscore and decode the url\n",
    "\n",
    "First we define a function that corrects the strings to have a readable format. For example, '%C3%89douard_Manet' is transformed to 'Édouard Manet'.\n",
    "\n",
    "We will create a function to decode a word, and we will be able to use it in series and dataframes using apply()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_word(word):\n",
    "    word = word.replace('_', ' ')\n",
    "    return unquote(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['articles'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles['articles'].apply(decode_word).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Distance Model\n",
    "We create a function that returns the semantic similarity between two words you provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the model outside the function (make sure to run this before using the function)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get embeddings (just because we will use it in semantic similarity function)\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Semantic similarity function\n",
    "def semantic_similarity(word1, word2):\n",
    "    embedding1 = get_embedding(word1)\n",
    "    embedding2 = get_embedding(word2)\n",
    "    return cosine_similarity(embedding1.detach().numpy(), embedding2.detach().numpy())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_similarity('king', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_similarity('king', 'chemistry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Distance Matrix\n",
    "Provided a series, it creates a df where indices and column names are the strings of the series, and fills the matrix with the semantic similarity between all words in the provided series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarity_matrix(titles):\n",
    "    df = pd.DataFrame(index=titles, columns=titles)\n",
    "    for i in range(len(titles)):\n",
    "        for j in range(i+1, len(titles)):\n",
    "            embedding1 = get_embedding(titles[i])\n",
    "            embedding2 = get_embedding(titles[j])\n",
    "            similarity = cosine_similarity(embedding1.detach().numpy(), embedding2.detach().numpy())[0][0]\n",
    "            df.iloc[i, j] = similarity\n",
    "            df.iloc[j, i] = similarity  # Copy value to lower triangle\n",
    "            np.fill_diagonal(df.values, 1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_similarity_matrix(pd.Series(['king', 'queen', 'chemistry', 'biology']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. Basic AI\n",
    "\n",
    "Most of the model requires for there to be an AI model we can compare it against.\n",
    "\n",
    "We were indicated by the TA to not focus on this too much, as this is a data analysis course, not an ML course. Because of this, we took the implementation of A\\* that was included in networkX and created two modified versions that now do the following:\n",
    "- First version returns all of the explored nodes, not just the shortest path found\n",
    "- Second version is forced to do a depth first search without being able to return\n",
    "\n",
    "For our purposes, the explored nodes is the most interesting metric, as it describes what were the links \"clicked\".\n",
    "\n",
    "We found a paper that implements a more complicated version, and we might be able to do something with graph neural networks, but for now this is good enough.\n",
    "\n",
    "We additionally do a bit of work to show how the system works timewise, as well as how the comparison will work in the future.\n",
    "\n",
    "The time comparison is not super useful though, as that will depend on hardware too much to be worth using easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import machine_searchers\n",
    "import time\n",
    "\n",
    "def modded_get_embedding(text: str):\n",
    "    temp_str = text.replace('_', ' ')\n",
    "    temp_str = unquote(temp_str)\n",
    "    inputs = tokenizer(temp_str, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "def distance_two_words(w1: str, w2: str):\n",
    "    \"\"\"Receives a string that was in the wikispeedia dataset, and transforms it as needed to work\n",
    "    with the berd embeddings.\"\"\"\n",
    "\n",
    "    embedding1 = modded_get_embedding(w1)\n",
    "    embedding2 = modded_get_embedding(w2)\n",
    "    similarity = cosine_similarity(embedding1.detach().numpy(), embedding2.detach().numpy())[0][0]\n",
    "    # Adding absolute, just in case it is needed\n",
    "    # Similarity is actually 1 - abs(similarity) + 1,\n",
    "    # As we want closer words to have a smaller distance\n",
    "    # The last plus one is to indicate that there would be an extra cost to exploring, as if not the system often\n",
    "    # thinks that there are nodes that have a distance of 0.5 or something like that\n",
    "    similarity = 1 - abs(similarity) + 1\n",
    "    # print(\"First word:\", w1, \". Second word:\", w2, \". GoodDistance:\", similarity)\n",
    "    return similarity\n",
    "\n",
    "start_time = time.time()\n",
    "lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(wikispeedia, 'Actor', 'Japan', heuristic=distance_two_words)\n",
    "end_time = time.time()\n",
    "\n",
    "# It's len - 1 because the target node is also included, and that node wasn't explored\n",
    "print(\"Using the modded a star that returns explored nodes:\")\n",
    "print(\" Found solution for Actor to Japan exploring the following number of nodes:\", len(lib_explore_1)-1)\n",
    "print(\" Found it in:\", end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(wikispeedia, 'Actor', 'Japan', heuristic=distance_two_words)\n",
    "end_time = time.time()\n",
    "\n",
    "# It's len - 1 because the target node is also included, and that node wasn't explored\n",
    "print(\"Using depth first only A star that returns explored nodes:\")\n",
    "print(\" Found solution for Actor to Japan exploring the following number of nodes:\", len(lib_explore_1)-1)\n",
    "print(\" Found it in:\", end_time-start_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we'll take the most commonly explored node pair path, run it through the two algorithms and see what is the result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# So it's finding the length between asteroid and viking\n",
    "article_combinations.sort_values('count', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "lib_path_1, lib_explore_1 = machine_searchers.modded_astar_path(wikispeedia, 'Asteroid', 'Viking', heuristic=distance_two_words)\n",
    "end_time = time.time()\n",
    "\n",
    "# It's len - 1 because the target node is also included, and that node wasn't explored\n",
    "print(\"Using the modded a star that returns explored nodes:\")\n",
    "print(\" Found solution for Asteroid to Viking exploring the following number of nodes:\", len(lib_explore_1)-1)\n",
    "print(\"Path length was:\", len(lib_path_1)-1)\n",
    "print(\" Found it in:\", end_time-start_time)\n",
    "\n",
    "start_time = time.time()\n",
    "lib_path_2, lib_explore_2 = machine_searchers.only_depth_first_astar_path(wikispeedia, 'Asteroid', 'Viking', heuristic=distance_two_words)\n",
    "end_time = time.time()\n",
    "print('')\n",
    "\n",
    "# It's len - 1 because the target node is also included, and that node wasn't explored\n",
    "print(\"Using depth first only A star that returns explored nodes:\")\n",
    "print(\" Found solution for Asteroid to Viking exploring the following number of nodes:\", len(lib_explore_2)-1)\n",
    "print(\"Path length was:\", len(lib_path_2)-1)\n",
    "print(\" Found it in:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lib_path_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So, based on the previous example of the path, this works out well enough. The systems spends a lot of time exploring and going back, which might be a common issue. There is a huge disconnect between explored and actual path length, but that is common for A\\*, so it's an expected caveat.\n",
    "\n",
    "It is interesting to note that the optimal solution passed through Paris, which seems to fit the definition of being one of the hubs that are described in the paper. Maybe the hub strategy is actually useful in most cases!\n",
    "\n",
    "The depth first method took a lot longer to run than planned. Based on this, it might be worth considering other alternatives.\n",
    "\n",
    "But still, at least we've proven the model works, and can give results that we can compare against humans. Again, this is using a much simpler method, but this could be enhanced in the future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
