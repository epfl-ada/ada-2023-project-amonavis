{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# What\n",
    "Our code is a mess. This is just to not have to run everything all the time. Allows us to focus on the data we have instead of trying to get even more messy data.\n",
    "\n",
    "This should be used as a default and copies should be done to build on top. The idea of this is that it's a basic AF readers so that actual analysis can be done."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "289176a3d9e45916"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "# networkx\n",
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import graphviz_layout\n",
    "\n",
    "# For semantic similarity\n",
    "from urllib.parse import unquote\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Python functions in .py file to read data\n",
    "import data_readers\n",
    "\n",
    "import warnings\n",
    "from tqdm import TqdmWarning\n",
    "warnings.filterwarnings('ignore', category=TqdmWarning)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a96a96f240a67cd3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# The links and edges\n",
    "wikispeedia = data_readers.read_wikispeedia_graph()\n",
    "\n",
    "# The finished paths\n",
    "finished_paths = data_readers.read_finished_paths()\n",
    "\n",
    "# The unfinished paths\n",
    "unfinished_paths = data_readers.read_unfinished_paths()\n",
    "\n",
    "# DF of all articles\n",
    "articles = data_readers.read_articles()\n",
    "\n",
    "# DF of all articles and their categories\n",
    "categories = data_readers.read_categories()\n",
    "\n",
    "# Searching for the string of a given article. It has to be formatted like the article name\n",
    "# Which shouldn't be a problem, as we'll probably usually retrieve them internally\n",
    "obi_wan_text = data_readers.plaintext_article_finder('Obi-Wan_Kenobi')\n",
    "\n",
    "source_target_info = data_readers.source_target_paths_information()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "893fb1e3c988e06e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# This is a last step, of making these three datasets have consistent column names and order\n",
    "# as well as guaranteeing that the index order is conistent with both elements\n",
    "# Even though the boolean column is weird, it's a boolean so that's all good\n",
    "\n",
    "# FUCK IT. We don't touch the decoded version, that's too painful!\n",
    "\n",
    "col_names = ['source', 'target', 'reached', 'length', 'visited']\n",
    "\n",
    "import ast\n",
    "\n",
    "carlos_df = pd.read_csv('machine_outputs/data_carlos.csv')\n",
    "carlos_df.columns = col_names\n",
    "\n",
    "carlos_df['visited'] = carlos_df['visited'].str.strip('\\\\').str.strip('{}')\n",
    "carlos_df['visited'] = carlos_df['visited'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "carol_df = pd.read_csv('machine_outputs/carolOutput.csv')\n",
    "carol_df.columns = col_names\n",
    "\n",
    "carol_df['visited'] = carol_df['visited'].str.strip('\\\\').str.strip('{}')\n",
    "carol_df['visited'] = carol_df['visited'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "landmark_df = pd.read_csv('machine_outputs/landmark_method_results.csv')\n",
    "\n",
    "# Landmark_df is the only one with substantive change\n",
    "landmark_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "landmark_df['reached'] = True\n",
    "landmark_df.columns = ['source', 'target', 'length', 'visited', 'reached']\n",
    "landmark_df = landmark_df[col_names]\n",
    "# Due to the way length was counted here, I need to update to increase by one\n",
    "landmark_df['length'] = landmark_df['length'] + 1\n",
    "\n",
    "landmark_df['visited'] = landmark_df['visited'].str.strip('\\\\').str.strip('{}')\n",
    "landmark_df['visited'] = landmark_df['visited'].apply(lambda x: ast.literal_eval(x))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ecc9d58c52c2b3e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
